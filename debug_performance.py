#!/usr/bin/env python3
"""
Performance Debug Script
This script will help identify bottlenecks in the RAG system.
"""

import time
import asyncio
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import AsyncOpenAI
from config import *

async def debug_rag_performance():
    """Debug each step of the RAG process to find bottlenecks"""
    print("ðŸ” DocBoss Performance Debug")
    print("=" * 50)
    
    # Test question
    question = "mÃ¼hendislik bÃ¶lÃ¼mleri neler?"
    
    # Step 1: Check GPU
    print(f"ðŸŽ® GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ðŸš€ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ðŸ”‹ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Step 2: Load models and measure time
    print("\nðŸ“Š Model Loading Times:")
    
    device = "cuda" if USE_GPU and torch.cuda.is_available() else "cpu"
    print(f"ðŸŽ¯ Using device: {device}")
    
    # Embedding model loading
    start_time = time.time()
    embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)
    embedding_load_time = time.time() - start_time
    print(f"âš¡ Embedding model load: {embedding_load_time:.2f}s")
    
    # Cross-encoder loading
    start_time = time.time()
    cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL, device=device)
    cross_encoder_load_time = time.time() - start_time
    print(f"âš¡ Cross-encoder load: {cross_encoder_load_time:.2f}s")
    
    # OpenAI client
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    
    # Step 3: Test embedding generation
    print("\nðŸ§ª Embedding Generation Test:")
    
    test_texts = [
        "Bu bir test cÃ¼mlesidir.",
        "MÃ¼hendislik bÃ¶lÃ¼mleri hakkÄ±nda bilgi arÄ±yorum.",
        "Ãœniversite bÃ¶lÃ¼mleri ve programlarÄ±",
        question
    ]
    
    # CPU test
    if device == "cuda":
        cpu_model = SentenceTransformer(EMBEDDING_MODEL, device='cpu')
        start_time = time.time()
        cpu_embeddings = cpu_model.encode(test_texts)
        cpu_time = time.time() - start_time
        print(f"ðŸ’» CPU embeddings: {cpu_time:.3f}s")
        del cpu_model
    
    # GPU test
    start_time = time.time()
    gpu_embeddings = embedding_model.encode(test_texts)
    gpu_time = time.time() - start_time
    print(f"ðŸŽ® GPU embeddings: {gpu_time:.3f}s")
    
    if device == "cuda" and 'cpu_time' in locals():
        speedup = cpu_time / gpu_time
        print(f"ðŸš€ GPU Speedup: {speedup:.2f}x")
    
    # Step 4: Test cross-encoder
    print("\nðŸŽ¯ Cross-Encoder Test:")
    
    test_pairs = [
        (question, "MÃ¼hendislik bÃ¶lÃ¼mleri Ã¶ÄŸrencilere teknik eÄŸitim saÄŸlar"),
        (question, "Ãœniversitede Ã§eÅŸitli mÃ¼hendislik programlarÄ± bulunur"),
        (question, "Bilgisayar mÃ¼hendisliÄŸi popÃ¼ler bir bÃ¶lÃ¼mdÃ¼r")
    ]
    
    start_time = time.time()
    scores = cross_encoder.predict(test_pairs)
    cross_encoder_time = time.time() - start_time
    print(f"âš¡ Cross-encoder scoring: {cross_encoder_time:.3f}s")
    print(f"ðŸ“Š Scores: {scores}")
    
    # Step 5: Test HyDE generation
    print("\nðŸ“ HyDE Generation Test:")
    
    start_time = time.time()
    hyde_prompt = f"""Åžu soruya verilen kapsamlÄ± bir cevap yaz: {question}
    
    Cevap:"""
    
    response = await openai_client.chat.completions.create(
        model=HYDE_LLM_MODEL,
        messages=[{"role": "user", "content": hyde_prompt}],
        temperature=0.1,
        max_tokens=200
    )
    
    hyde_time = time.time() - start_time
    hyde_answer = response.choices[0].message.content.strip()
    print(f"âš¡ HyDE generation: {hyde_time:.3f}s")
    print(f"ðŸ“ HyDE answer: {hyde_answer[:100]}...")
    
    # Step 6: Test final response generation
    print("\nðŸ’¬ Final Response Test:")
    
    mock_context = """
    Ankara Bilim Ãœniversitesi'nde ÅŸu mÃ¼hendislik bÃ¶lÃ¼mleri bulunmaktadÄ±r:
    - Bilgisayar MÃ¼hendisliÄŸi
    - EndÃ¼stri MÃ¼hendisliÄŸi  
    - Elektrik-Elektronik MÃ¼hendisliÄŸi
    """
    
    start_time = time.time()
    
    final_response = await openai_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": PROMPT_TEMPLATE.format(context=mock_context, question=question)}],
        temperature=0.1,
        max_tokens=500
    )
    
    response_time = time.time() - start_time
    final_answer = final_response.choices[0].message.content.strip()
    print(f"âš¡ Final response: {response_time:.3f}s")
    print(f"ðŸ’¬ Answer: {final_answer[:100]}...")
    
    # Summary
    print("\nðŸ“ˆ Performance Summary:")
    print(f"ðŸ¤– Embedding model load: {embedding_load_time:.2f}s")
    print(f"ðŸŽ¯ Cross-encoder load: {cross_encoder_load_time:.2f}s")
    print(f"ðŸ§  Embedding generation: {gpu_time:.3f}s")
    print(f"âš–ï¸ Cross-encoder scoring: {cross_encoder_time:.3f}s")
    print(f"ðŸ“ HyDE generation: {hyde_time:.3f}s")
    print(f"ðŸ’¬ Final response: {response_time:.3f}s")
    
    total_ai_time = hyde_time + response_time
    total_ml_time = gpu_time + cross_encoder_time
    
    print(f"\nðŸŽ¯ Bottleneck Analysis:")
    print(f"ðŸ”¥ Total AI API time: {total_ai_time:.2f}s")
    print(f"âš¡ Total ML processing: {total_ml_time:.2f}s")
    
    if total_ai_time > total_ml_time * 2:
        print("ðŸš¨ BOTTLENECK: OpenAI API calls are the main bottleneck!")
    elif total_ml_time > 3.0:
        print("ðŸš¨ BOTTLENECK: ML processing (GPU optimization needed)")
    else:
        print("âœ… Performance looks good!")

if __name__ == "__main__":
    asyncio.run(debug_rag_performance()) 
 
"""
Performance Debug Script
This script will help identify bottlenecks in the RAG system.
"""

import time
import asyncio
import torch
from sentence_transformers import SentenceTransformer, CrossEncoder
from openai import AsyncOpenAI
from config import *

async def debug_rag_performance():
    """Debug each step of the RAG process to find bottlenecks"""
    print("ðŸ” DocBoss Performance Debug")
    print("=" * 50)
    
    # Test question
    question = "mÃ¼hendislik bÃ¶lÃ¼mleri neler?"
    
    # Step 1: Check GPU
    print(f"ðŸŽ® GPU Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"ðŸš€ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ðŸ”‹ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Step 2: Load models and measure time
    print("\nðŸ“Š Model Loading Times:")
    
    device = "cuda" if USE_GPU and torch.cuda.is_available() else "cpu"
    print(f"ðŸŽ¯ Using device: {device}")
    
    # Embedding model loading
    start_time = time.time()
    embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)
    embedding_load_time = time.time() - start_time
    print(f"âš¡ Embedding model load: {embedding_load_time:.2f}s")
    
    # Cross-encoder loading
    start_time = time.time()
    cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL, device=device)
    cross_encoder_load_time = time.time() - start_time
    print(f"âš¡ Cross-encoder load: {cross_encoder_load_time:.2f}s")
    
    # OpenAI client
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    
    # Step 3: Test embedding generation
    print("\nðŸ§ª Embedding Generation Test:")
    
    test_texts = [
        "Bu bir test cÃ¼mlesidir.",
        "MÃ¼hendislik bÃ¶lÃ¼mleri hakkÄ±nda bilgi arÄ±yorum.",
        "Ãœniversite bÃ¶lÃ¼mleri ve programlarÄ±",
        question
    ]
    
    # CPU test
    if device == "cuda":
        cpu_model = SentenceTransformer(EMBEDDING_MODEL, device='cpu')
        start_time = time.time()
        cpu_embeddings = cpu_model.encode(test_texts)
        cpu_time = time.time() - start_time
        print(f"ðŸ’» CPU embeddings: {cpu_time:.3f}s")
        del cpu_model
    
    # GPU test
    start_time = time.time()
    gpu_embeddings = embedding_model.encode(test_texts)
    gpu_time = time.time() - start_time
    print(f"ðŸŽ® GPU embeddings: {gpu_time:.3f}s")
    
    if device == "cuda" and 'cpu_time' in locals():
        speedup = cpu_time / gpu_time
        print(f"ðŸš€ GPU Speedup: {speedup:.2f}x")
    
    # Step 4: Test cross-encoder
    print("\nðŸŽ¯ Cross-Encoder Test:")
    
    test_pairs = [
        (question, "MÃ¼hendislik bÃ¶lÃ¼mleri Ã¶ÄŸrencilere teknik eÄŸitim saÄŸlar"),
        (question, "Ãœniversitede Ã§eÅŸitli mÃ¼hendislik programlarÄ± bulunur"),
        (question, "Bilgisayar mÃ¼hendisliÄŸi popÃ¼ler bir bÃ¶lÃ¼mdÃ¼r")
    ]
    
    start_time = time.time()
    scores = cross_encoder.predict(test_pairs)
    cross_encoder_time = time.time() - start_time
    print(f"âš¡ Cross-encoder scoring: {cross_encoder_time:.3f}s")
    print(f"ðŸ“Š Scores: {scores}")
    
    # Step 5: Test HyDE generation
    print("\nðŸ“ HyDE Generation Test:")
    
    start_time = time.time()
    hyde_prompt = f"""Åžu soruya verilen kapsamlÄ± bir cevap yaz: {question}
    
    Cevap:"""
    
    response = await openai_client.chat.completions.create(
        model=HYDE_LLM_MODEL,
        messages=[{"role": "user", "content": hyde_prompt}],
        temperature=0.1,
        max_tokens=200
    )
    
    hyde_time = time.time() - start_time
    hyde_answer = response.choices[0].message.content.strip()
    print(f"âš¡ HyDE generation: {hyde_time:.3f}s")
    print(f"ðŸ“ HyDE answer: {hyde_answer[:100]}...")
    
    # Step 6: Test final response generation
    print("\nðŸ’¬ Final Response Test:")
    
    mock_context = """
    Ankara Bilim Ãœniversitesi'nde ÅŸu mÃ¼hendislik bÃ¶lÃ¼mleri bulunmaktadÄ±r:
    - Bilgisayar MÃ¼hendisliÄŸi
    - EndÃ¼stri MÃ¼hendisliÄŸi  
    - Elektrik-Elektronik MÃ¼hendisliÄŸi
    """
    
    start_time = time.time()
    
    final_response = await openai_client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": PROMPT_TEMPLATE.format(context=mock_context, question=question)}],
        temperature=0.1,
        max_tokens=500
    )
    
    response_time = time.time() - start_time
    final_answer = final_response.choices[0].message.content.strip()
    print(f"âš¡ Final response: {response_time:.3f}s")
    print(f"ðŸ’¬ Answer: {final_answer[:100]}...")
    
    # Summary
    print("\nðŸ“ˆ Performance Summary:")
    print(f"ðŸ¤– Embedding model load: {embedding_load_time:.2f}s")
    print(f"ðŸŽ¯ Cross-encoder load: {cross_encoder_load_time:.2f}s")
    print(f"ðŸ§  Embedding generation: {gpu_time:.3f}s")
    print(f"âš–ï¸ Cross-encoder scoring: {cross_encoder_time:.3f}s")
    print(f"ðŸ“ HyDE generation: {hyde_time:.3f}s")
    print(f"ðŸ’¬ Final response: {response_time:.3f}s")
    
    total_ai_time = hyde_time + response_time
    total_ml_time = gpu_time + cross_encoder_time
    
    print(f"\nðŸŽ¯ Bottleneck Analysis:")
    print(f"ðŸ”¥ Total AI API time: {total_ai_time:.2f}s")
    print(f"âš¡ Total ML processing: {total_ml_time:.2f}s")
    
    if total_ai_time > total_ml_time * 2:
        print("ðŸš¨ BOTTLENECK: OpenAI API calls are the main bottleneck!")
    elif total_ml_time > 3.0:
        print("ðŸš¨ BOTTLENECK: ML processing (GPU optimization needed)")
    else:
        print("âœ… Performance looks good!")

if __name__ == "__main__":
    asyncio.run(debug_rag_performance()) 
 
 