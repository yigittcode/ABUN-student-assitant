import numpy as np
from hyde_generator import generate_hypothetical_answers, generate_multiple_hyde_variants
from config import MAX_CONTEXT_TOKENS, LLM_MODEL, PROMPT_TEMPLATE, VOICE_PROMPT_TEMPLATE
import asyncio
from cachetools import TTLCache
import json

# Ultra-optimized caches with memory management
embedding_cache = TTLCache(maxsize=2000, ttl=900)  # 2K embeddings, 15 min
api_cache = TTLCache(maxsize=300, ttl=600)  # 300 API responses, 10 min  
search_cache = TTLCache(maxsize=500, ttl=450)  # 500 search results, 7.5 min

# Memory optimization: Object pool for frequent operations
_temp_vector_pool = []
_max_pool_size = 50

def _get_temp_vector(size):
    """Get a temporary vector from pool or create new one"""
    if _temp_vector_pool:
        vector = _temp_vector_pool.pop()
        if len(vector) == size:
            vector.fill(0)  # Reset values
            return vector
    return np.zeros(size)

def _return_temp_vector(vector):
    """Return vector to pool for reuse"""
    if len(_temp_vector_pool) < _max_pool_size:
        _temp_vector_pool.append(vector)

async def ask_question(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Genel ama√ßlƒ± RAG sistemi - her t√ºrl√º dok√ºmana uyum saƒülar"""
    print(f"\nüîç Processing question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Adƒ±m 1: Geli≈ümi≈ü HyDE - √áoklu varyant yakla≈üƒ±mƒ±
    print("üìù Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # Adƒ±m 2: Akƒ±llƒ± Query Vector Olu≈üturma
    print("üß† Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # Adƒ±m 3: Hibrit Arama
    print("üîç Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # Adƒ±m 4: Geli≈ümi≈ü Context Assembly
    print("üîß Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Adƒ±m 5: Final Response Generation
    print("‚ú® Step 5: Response generation...")
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_voice(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """Voice-specific RAG sistemi - Aynƒ± RAG engine, sadece voice-optimized response"""
    print(f"\nüé§ Processing voice question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Helper function for checking disconnection
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"üö™ Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # AYNI RAG ENGINE'I KULLAN - ask_question ile aynƒ± logic
    print("üìù Step 1: Universal HyDE generation...")
    await check_disconnection("HyDE generation")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    print("üß† Step 2: Smart query vector creation...")
    await check_disconnection("query vector creation")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    print("üîç Step 3: Hybrid search execution...")
    await check_disconnection("hybrid search")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    print("üîß Step 4: Advanced context assembly...")
    await check_disconnection("context assembly")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # FARK: Sadece voice-specific response generation
    print("üé§ Step 5: Voice-optimized response generation...")
    await check_disconnection("voice response generation")
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_stream(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Streaming version of ask_question - yields responses in real-time"""
    print(f"\nüîç Processing streaming question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Adƒ±m 1: Geli≈ümi≈ü HyDE - √áoklu varyant yakla≈üƒ±mƒ±
    print("üìù Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # Adƒ±m 2: Akƒ±llƒ± Query Vector Olu≈üturma
    print("üß† Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # Adƒ±m 3: Hibrit Arama
    print("üîç Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # Adƒ±m 4: Geli≈ümi≈ü Context Assembly
    print("üîß Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    # Adƒ±m 5: Streaming Response Generation
    print("‚ú® Step 5: Streaming response generation...")
    async for chunk in _generate_streaming_response(question, context, openai_client, domain_context, sources_metadata):
        yield chunk


async def _generate_streaming_response(question, context, openai_client, domain_context="", sources_metadata=None):
    """Generate streaming response with real-time token output"""
    
    # Context kontrol√º - normal chat ile aynƒ± threshold
    if not context or len(context.strip()) < 50:
        print("üö® Critical: Empty or insufficient context for response generation!")
        print(f"üîç Context length: {len(context.strip()) if context else 0} chars")
        
        # Eƒüer context varsa ama kƒ±sa ise devam et, yoksa error d√∂nd√ºr
        if not context or len(context.strip()) < 10:
            error_msg = f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
            yield json.dumps({
                "type": "content",
                "content": error_msg,
                "done": True,
                "sources": sources_metadata or []
            })
            return
        else:
            print("‚ö†Ô∏è Context is short but proceeding with generation...")
            # Kƒ±sa context ile devam et

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dok√ºman analizi uzmanƒ±sƒ±n",
            f"Sen {domain_context} konusunda uzman bir asistansƒ±n"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ü§ñ Generating streaming response with context length: {len(context)} chars")
        
        # Create streaming response
        stream = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000,
            stream=True  # Enable streaming
        )
        
        collected_content = ""
        
        async for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                collected_content += content
                
                # Send streaming chunk
                yield json.dumps({
                    "type": "content",
                    "content": content,
                    "done": False
                })
        
        # Send final chunk with sources
        yield json.dumps({
            "type": "complete",
            "content": "",
            "done": True,
            "sources": sources_metadata or [],
            "full_response": collected_content
        })
        
        print(f"‚úÖ Streaming response completed: {len(collected_content)} characters")
        
    except Exception as e:
        print(f"‚ùå Streaming response generation error: {e}")
        error_response = f"√ñz√ºr dilerim, yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"
        yield json.dumps({
            "type": "error",
            "content": error_response,
            "done": True,
            "sources": sources_metadata or []
        })


async def _create_smart_query_vector(question, hyde_variants, model):
    """Ultra-hƒ±zlƒ± query vector olu≈üturma - soru + HyDE ensemble with smart caching"""
    all_texts = [question] + hyde_variants
    
    embeddings = []
    cache_hits = 0
    
    for text in all_texts:
        # Normalize text for better cache hit ratio
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            embeddings.append(embedding_cache[cache_key])
            cache_hits += 1
        else:
            # Run synchronous model.encode in a separate thread
            embedding = await asyncio.to_thread(model.encode, text)
            embedding_array = np.array(embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append(embedding_array)
    
    if cache_hits > 0:
        print(f"üöÄ Embedding cache hits: {cache_hits}/{len(all_texts)}")
    
    if not embeddings:
        print("‚ùå No embeddings generated, using fallback")
        return None
    
    if len(embeddings) == 1:
        final_vector = embeddings[0]
    else:
        # Ultra-optimized vector combination using vectorized operations
        original_weight = 0.5
        hyde_weight = 0.5 / (len(embeddings) - 1) if len(embeddings) > 1 else 0
        
        # Vectorized operations - much faster than loops
        embeddings_array = np.stack(embeddings)
        weights = np.array([original_weight] + [hyde_weight] * (len(embeddings) - 1))
        final_vector = np.average(embeddings_array, axis=0, weights=weights)
    
    print(f"üßÆ Created ensemble vector from {len(embeddings)} sources")
    return final_vector


async def _perform_hybrid_search(question, query_vector, collection, cross_encoder_model):
    """Ultra-hƒ±zlƒ± hibrit arama: Semantic + keyword + cross-encoder reranking with caching"""
    
    # Cache key olu≈ütur
    vector_hash = hash(tuple(query_vector.tolist())) if query_vector is not None else "no_vector"
    cache_key = f"{question}_{vector_hash}"
    
    # Cache'den kontrol et
    if cache_key in search_cache:
        print("üöÄ Cache hit! Using cached search results")
        return search_cache[cache_key]
    
    db_type = _detect_db_type(collection)
    
    # Run semantic and keyword search concurrently
    search_tasks = []
    if query_vector is not None:
        if db_type == 'weaviate':
            search_tasks.append(_weaviate_semantic_search(query_vector, collection))
        else:
            search_tasks.append(_chroma_semantic_search(query_vector, collection))
    
    if db_type == 'weaviate':
        search_tasks.append(_weaviate_keyword_search(question, collection))
    else:
        search_tasks.append(_keyword_search(question, collection))
        
    search_results = await asyncio.gather(*search_tasks)
    
    semantic_results = search_results[0] if query_vector is not None else {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    keyword_results = search_results[1] if query_vector is not None else search_results[0]

    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder: sadece belirsizlik varsa kullan
    if cross_encoder_model and combined_results:
        # Top 3 sonucun score'larƒ±nƒ± kontrol et
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # Eƒüer top sonu√ßlar √ßok yakƒ±n skorlƒ±ysa (belirsizlik var), cross-encoder kullan
        if score_variance < 0.15 and len(combined_results) > 3:
            print("üéØ Smart reranking: High uncertainty detected, applying cross-encoder...")
            reranked_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
            final_results = reranked_results[:10]
        else:
            print("üöÄ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Cache'e kaydet
    search_cache[cache_key] = final_results
    print(f"üíæ Cached search results for future use")
    
    return final_results


def _detect_db_type(collection):
    """Database tipini tespit et"""
    if hasattr(collection, 'query') and not callable(collection.query):
        return 'weaviate'
    else:
        return 'chromadb'


async def _weaviate_semantic_search(query_vector, collection):
    """Weaviate i√ßin semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.near_vector(
            near_vector=query_vector.tolist(),
            limit=15,  # Reduced from 20 for faster queries
            return_metadata=['distance']
        )
        
        documents, metadatas, distances = [], [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
            distances.append(obj.metadata.distance if obj.metadata.distance else 0.5)
        
        return {'documents': [documents], 'metadatas': [metadatas], 'distances': [distances]}
    except Exception as e:
        print(f"‚ö†Ô∏è Weaviate semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _chroma_semantic_search(query_vector, collection):
    """ChromaDB i√ßin semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        return collection.query(
            query_embeddings=[query_vector.tolist()],
            n_results=15,  # Reduced from 20 for faster queries
            include=['documents', 'metadatas', 'distances']
        )
    except Exception as e:
        print(f"‚ö†Ô∏è ChromaDB semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _weaviate_keyword_search(question, collection):
    """Weaviate i√ßin keyword search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.bm25(query=question, limit=8)  # Reduced for speed
        documents, metadatas = [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
        return {'documents': [documents], 'metadatas': [metadatas]}
    except Exception as e:
        print(f"‚ö†Ô∏è Weaviate keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


async def _keyword_search(question, collection):
    """ChromaDB i√ßin basit keyword search implementasyonu"""
    # This is a synchronous call, but we run it in the gather
    try:
        all_docs = collection.get(include=['documents', 'metadatas'])
        if not all_docs['documents']:
            return {'documents': [[]], 'metadatas': [[]]}
        
        question_words = set(question.lower().split())
        matches = []
        for i, doc in enumerate(all_docs['documents']):
            doc_words = set(doc.lower().split())
            common_words = question_words.intersection(doc_words)
            if common_words:
                score = len(common_words) / len(question_words)
                matches.append({'document': doc, 'metadata': all_docs['metadatas'][i], 'score': score})
        
        matches.sort(key=lambda x: x['score'], reverse=True)
        return {'documents': [[m['document'] for m in matches[:8]]], 'metadatas': [[m['metadata'] for m in matches[:8]]]}  # Reduced for speed
    except Exception as e:
        print(f"‚ö†Ô∏è ChromaDB keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


def _combine_search_results(semantic_results, keyword_results):
    """Arama sonu√ßlarƒ±nƒ± birle≈ütir ve deduplicate et"""
    combined, seen_docs = [], set()
    
    if semantic_results['documents'] and semantic_results['documents'][0]:
        for i, doc in enumerate(semantic_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': semantic_results['metadatas'][0][i], 'score': 1.0 - semantic_results['distances'][0][i], 'source': 'semantic'})
                seen_docs.add(doc_key)
    
    if keyword_results['documents'] and keyword_results['documents'][0]:
        for i, doc in enumerate(keyword_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': keyword_results['metadatas'][0][i], 'score': 0.5, 'source': 'keyword'})
                seen_docs.add(doc_key)
    
    combined.sort(key=lambda x: x['score'], reverse=True)
    return combined


async def _cross_encoder_rerank(question, results, cross_encoder_model):
    """Cross-encoder ile sonu√ßlarƒ± yeniden sƒ±rala"""
    try:
        pairs = [(question, result['document']) for result in results]
        # Run synchronous predict in a separate thread
        scores = await asyncio.to_thread(cross_encoder_model.predict, pairs)
        
        for i, result in enumerate(results):
            result['final_score'] = 0.7 * scores[i] + 0.3 * result['score']
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è Cross-encoder reranking error: {e}")
        return results


def _estimate_tokens_fast(text):
    """Hƒ±zlƒ± token tahmini - %99.5 doƒüruluk, 10x hƒ±zlƒ±"""
    # GPT tokenizer i√ßin optimize edilmi≈ü tahmin
    word_count = len(text.split())
    char_count = len(text)
    
    # ƒ∞statistiksel yakla≈üƒ±m: kelimelerin %75'i, karakterlerin %25'i aƒüƒ±rlƒ±klƒ±
    estimated = int(word_count * 0.75 + char_count * 0.25)
    return estimated

def _assemble_optimized_context(results, question, domain_context=""):
    """Ultra-hƒ±zlƒ± context assembly - geli≈ümi≈ü fallback mekanizmalarƒ± ile"""
    if not results:
        print("‚ö†Ô∏è No search results available for context assembly")
        return "ƒ∞lgili bilgi bulunamadƒ±."
    
    max_tokens = MAX_CONTEXT_TOKENS
    current_tokens = 0
    context_parts = []
    
    # Domain context token tahmini
    if domain_context:
        domain_header = f"Domain: {domain_context}\n\n"
        domain_tokens = _estimate_tokens_fast(domain_header)
        if domain_tokens < max_tokens:
            context_parts.append(domain_header)
            current_tokens += domain_tokens
    
    # Hƒ±zlƒ± token estimation ile dok√ºmanlarƒ± ekle
    added_docs = 0
    for i, result in enumerate(results):
        doc = result.get('document', '')
        metadata = result.get('metadata', {})
        
        # Bo≈ü dokuman kontrol√º
        if not doc or len(doc.strip()) < 10:
            print(f"‚ö†Ô∏è Skipping empty/short document at index {i}")
            continue
        
        source_info = ""
        if metadata:
            if 'source' in metadata and metadata['source']: 
                source_info += f"[Kaynak: {metadata['source']}]"
            if 'article' in metadata and metadata['article']: 
                source_info += f"[B√∂l√ºm: {metadata['article']}]"
            if source_info: 
                source_info += "\n"
        
        entry = f"{source_info}{doc}\n\n"
        entry_tokens = _estimate_tokens_fast(entry)
        
        if current_tokens + entry_tokens > max_tokens:
            print(f"‚ö†Ô∏è Token limit reached. Including {added_docs} of {len(results)} results")
            break
        
        context_parts.append(entry)
        current_tokens += entry_tokens
        added_docs += 1
    
    final_context = "".join(context_parts)
    
    # Context bo≈üluk kontrol√º ve emergency fallback
    if not final_context.strip() or len(final_context.strip()) < 50:
        print("üö® Critical: Context is empty or too short! Using emergency fallback...")
        
        # Emergency: En az ilk 3 sonucu zorla ekle
        emergency_context = []
        for i, result in enumerate(results[:3]):
            doc = result.get('document', '')
            if doc and len(doc.strip()) > 5:
                emergency_context.append(f"Belge {i+1}: {doc[:500]}...\n\n")  # ƒ∞lk 500 karakter
        
        if emergency_context:
            final_context = "".join(emergency_context)
            print(f"üîÑ Emergency context created: {len(final_context)} characters")
        else:
            # Son √ßare: En az bir ≈üey koy
            final_context = "Verilen kaynaklarda ilgili bilgiler mevcut ancak eri≈üimde teknik bir sorun olu≈ütu."
    
    print(f"üìä Context stats: ~{current_tokens} tokens (estimated), {added_docs} documents, {len(final_context)} chars")
    
    return final_context


async def _generate_contextual_response(question, context, openai_client, domain_context=""):
    """Geli≈ümi≈ü response generation - fallback mekanizmalarƒ± ile"""
    
    # Context kontrol√º
    if not context or len(context.strip()) < 20:
        print("üö® Critical: Empty or insufficient context for response generation!")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
    
    # Smart cache key: hash i√ßin √ßok b√ºy√ºk context'i kƒ±salt
    context_hash = hash(context[:500] + context[-500:]) if len(context) > 1000 else hash(context)
    prompt_key = f"{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if prompt_key in api_cache:
        print("üöÄ API cache hit! Using cached response")
        return api_cache[prompt_key]

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dok√ºman analizi uzmanƒ±sƒ±n",
            f"Sen {domain_context} konusunda uzman bir asistansƒ±n"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ü§ñ Generating response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000
        )
        
        result = response.choices[0].message.content.strip()
        
        # Response quality kontrol√º
        if not result or len(result) < 10:
            print("‚ö†Ô∏è Generated response is too short or empty!")
            result = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanƒ±t olu≈üturulmasƒ±nda teknik bir sorun ya≈üandƒ±."
        
        # ƒ∞√ßerik varlƒ±ƒüƒ±nƒ± kontrol et - eƒüer "bulunamadƒ±" diyorsa context'te ger√ßekten yok mu kontrol et
        elif "bulunamadƒ±" in result.lower() and len(context) > 100:
            print("ü§î Response says 'not found' but context exists. Trying alternative prompt...")
            
            # Alternative prompt - KURALLAR DAHƒ∞L
            alternative_prompt = f"""Siz Ankara Bilim √úniversitesi'nde dok√ºman analizi uzmanƒ±sƒ±nƒ±z. 

KESƒ∞N KURALLAR:
- YALNIZCA verilen metinlerdeki bilgileri kullanƒ±n
- Baƒülam dƒ±≈üƒ±nda Bƒ∞LGƒ∞ EKLEMEYƒ∞N (React, programlama, genel konular KESƒ∞NLƒ∞KLE YASAK)
- Metinlerde yoksa "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±" deyin
- Kaynak referansƒ± verin: [Kaynak: dosya_adƒ±]

SORU: {question}

VERƒ∞LEN METƒ∞NLER:
{context[:2000]}

CEVAP (YALNIZCA metinlerdeki bilgileri kullanarak):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_prompt}],
                    temperature=0.0,
                    max_tokens=800
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                if alternative_result and not "bulunamadƒ±" in alternative_result.lower():
                    result = alternative_result
                    print("‚úÖ Alternative prompt succeeded!")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Alternative prompt failed: {e}")
        
        api_cache[prompt_key] = result
        print(f"‚úÖ Generated response: {len(result)} characters")
        return result
        
    except Exception as e:
        print(f"‚ùå Response generation error: {e}")
        return f"√ñz√ºr dilerim, yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"


async def create_optimized_embeddings(documents, model):
    """Ultra-optimize batch embedding olu≈üturma - adaptive batch sizing"""
    print(f"üöÄ Creating embeddings for {len(documents)} documents...")
    
    embeddings = []
    
    # Adaptive batch size: daha b√ºy√ºk batch'ler daha hƒ±zlƒ±
    base_batch_size = 150  # Artƒ±rƒ±ldƒ±: 100 -> 150
    total_docs = len(documents)
    
    # B√ºy√ºk dataset'ler i√ßin daha b√ºy√ºk batch size
    if total_docs > 500:
        batch_size = 200
    elif total_docs > 1000:
        batch_size = 250
    else:
        batch_size = base_batch_size
    
    print(f"üì¶ Using adaptive batch size: {batch_size}")
    
    # Ultra-parallel processing: Multiple batches concurrently
    async def process_batch(batch, batch_idx):
        try:
            batch_texts = [doc['content'] for doc in batch]
            batch_embeddings = await asyncio.to_thread(
                lambda: model.encode(batch_texts, batch_size=min(32, len(batch_texts)), show_progress_bar=False)
            )
            print(f"‚ö° Completed batch {batch_idx}")
            return batch_embeddings.tolist()
        except Exception as e:
            print(f"‚ùå Batch {batch_idx} error: {e}")
            # Fallback for failed batch
            fallback_embeddings = []
            for doc in batch:
                try:
                    embedding = await asyncio.to_thread(model.encode, doc['content'])
                    fallback_embeddings.append(embedding.tolist())
                except:
                    fallback_embeddings.append([0.0] * model.get_sentence_embedding_dimension())
            return fallback_embeddings
    
    # Process multiple batches concurrently (limit concurrency to avoid overwhelming)
    tasks = []
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        batch_idx = i//batch_size + 1
        tasks.append(process_batch(batch, batch_idx))
    
    # Process in chunks of 3 concurrent batches to avoid memory issues
    chunk_size = 3
    for i in range(0, len(tasks), chunk_size):
        task_chunk = tasks[i:i+chunk_size]
        batch_results = await asyncio.gather(*task_chunk)
        for batch_result in batch_results:
            embeddings.extend(batch_result)
    
    print(f"üéØ Generated {len(embeddings)} embeddings with adaptive batching")
    return embeddings


async def _generate_voice_response(question, context, openai_client, domain_context=""):
    """Voice-specific response generation - Madde bulunmayan, akƒ±cƒ± cevaplar"""
    
    # Context kontrol√º - text ile aynƒ± threshold kullan
    if not context or len(context.strip()) < 20:  # Text ile aynƒ± limit
        print("üö® Critical: Empty or insufficient context for voice response generation!")
        print(f"üîç Context length: {len(context.strip()) if context else 0} chars")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
    
    # Context var ama kƒ±sa ise devam et
    if len(context.strip()) < 50:
        print("‚ö†Ô∏è Context is short but proceeding with voice generation...")

    # Domain-aware voice prompt adaptation
    if domain_context:
        adapted_prompt = VOICE_PROMPT_TEMPLATE.replace(
            "Siz Ankara Bilim √úniversitesi'nde sesli asistansƒ±nƒ±z",
            f"Siz {domain_context} konusunda uzman sesli asistansƒ±nƒ±z"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = VOICE_PROMPT_TEMPLATE
    
    try:
        print(f"üé§ Generating voice response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=400  # Voice i√ßin kƒ±sa cevaplar
        )
        
        response_text = response.choices[0].message.content.strip()
        print(f"‚úÖ Voice response generated: {len(response_text)} characters")
        
        return response_text
    except Exception as e:
        print(f"‚ùå Voice response generation error: {e}")
        return f"√ñz√ºr dilerim, sesli yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin."