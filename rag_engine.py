import asyncio
import uuid
from typing import List, Dict, Any, Optional
from config import LLM_MODEL, HYDE_LLM_MODEL
import numpy as np
from hyde_generator import generate_hypothetical_answers, generate_multiple_hyde_variants
from config import MAX_CONTEXT_TOKENS, PROMPT_TEMPLATE, VOICE_PROMPT_TEMPLATE
from cachetools import TTLCache
import json

# Ultra-optimized caches with memory management
embedding_cache = TTLCache(maxsize=2000, ttl=900)  # 2K embeddings, 15 min
api_cache = TTLCache(maxsize=300, ttl=600)  # 300 API responses, 10 min  
search_cache = TTLCache(maxsize=500, ttl=450)  # 500 search results, 7.5 min

# Memory optimization: Object pool for frequent operations
_temp_vector_pool = []
_max_pool_size = 50

def _get_temp_vector(size):
    """Get a temporary vector from pool or create new one"""
    if _temp_vector_pool:
        vector = _temp_vector_pool.pop()
        if len(vector) == size:
            vector.fill(0)  # Reset values
            return vector
    return np.zeros(size)

def _return_temp_vector(vector):
    """Return vector to pool for reuse"""
    if len(_temp_vector_pool) < _max_pool_size:
        _temp_vector_pool.append(vector)

async def ask_question(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Genel amaÃ§lÄ± RAG sistemi - her tÃ¼rlÃ¼ dokÃ¼mana uyum saÄŸlar"""
    print(f"\nğŸ” Processing question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # AdÄ±m 1: GeliÅŸmiÅŸ HyDE - Ã‡oklu varyant yaklaÅŸÄ±mÄ±
    print("ğŸ“ Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # AdÄ±m 2: AkÄ±llÄ± Query Vector OluÅŸturma
    print("ğŸ§  Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # AdÄ±m 3: Hibrit Arama
    print("ğŸ” Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # AdÄ±m 4: GeliÅŸmiÅŸ Context Assembly
    print("ğŸ”§ Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # AdÄ±m 5: Final Response Generation
    print("âœ¨ Step 5: Response generation...")
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_voice(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """Voice-specific RAG sistemi - AynÄ± RAG engine, sadece voice-optimized response"""
    print(f"\nğŸ¤ Processing voice question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # Helper function for checking disconnection
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"ğŸšª Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # AYNI RAG ENGINE'I KULLAN - ask_question ile aynÄ± logic
    print("ğŸ“ Step 1: Universal HyDE generation...")
    await check_disconnection("HyDE generation")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    print("ğŸ§  Step 2: Smart query vector creation...")
    await check_disconnection("query vector creation")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    print("ğŸ” Step 3: Hybrid search execution...")
    await check_disconnection("hybrid search")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    print("ğŸ”§ Step 4: Advanced context assembly...")
    await check_disconnection("context assembly")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # FARK: Sadece voice-specific response generation
    print("ğŸ¤ Step 5: Voice-optimized response generation...")
    await check_disconnection("voice response generation")
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_stream(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Streaming version of ask_question - yields responses in real-time"""
    print(f"\nğŸ” Processing streaming question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # AdÄ±m 1: GeliÅŸmiÅŸ HyDE - Ã‡oklu varyant yaklaÅŸÄ±mÄ±
    print("ğŸ“ Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # AdÄ±m 2: AkÄ±llÄ± Query Vector OluÅŸturma
    print("ğŸ§  Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # AdÄ±m 3: Hibrit Arama
    print("ğŸ” Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # AdÄ±m 4: GeliÅŸmiÅŸ Context Assembly
    print("ğŸ”§ Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    # AdÄ±m 5: Streaming Response Generation
    print("âœ¨ Step 5: Streaming response generation...")
    async for chunk in _generate_streaming_response(question, context, openai_client, domain_context, sources_metadata):
        yield chunk


async def _generate_streaming_response(question, context, openai_client, domain_context="", sources_metadata=None):
    """Generate streaming response with real-time token output"""
    
    # Context kontrolÃ¼ - normal chat ile aynÄ± threshold
    if not context or len(context.strip()) < 50:
        print("ğŸš¨ Critical: Empty or insufficient context for response generation!")
        print(f"ğŸ” Context length: {len(context.strip()) if context else 0} chars")
        
        # EÄŸer context varsa ama kÄ±sa ise devam et, yoksa error dÃ¶ndÃ¼r
        if not context or len(context.strip()) < 10:
            error_msg = f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
            yield json.dumps({
                "type": "content",
                "content": error_msg,
                "done": True,
                "sources": sources_metadata or []
            })
            return
        else:
            print("âš ï¸ Context is short but proceeding with generation...")
            # KÄ±sa context ile devam et

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dokÃ¼man analizi uzmanÄ±sÄ±n",
            f"Sen {domain_context} konusunda uzman bir asistansÄ±n"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤– Generating streaming response with context length: {len(context)} chars")
        
        # Create streaming response
        stream = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000,
            stream=True  # Enable streaming
        )
        
        collected_content = ""
        
        async for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                collected_content += content
                
                # Send streaming chunk
                yield json.dumps({
                    "type": "content",
                    "content": content,
                    "done": False
                })
        
        # Send final chunk with sources
        yield json.dumps({
            "type": "complete",
            "content": "",
            "done": True,
            "sources": sources_metadata or [],
            "full_response": collected_content
        })
        
        print(f"âœ… Streaming response completed: {len(collected_content)} characters")
        
    except Exception as e:
        print(f"âŒ Streaming response generation error: {e}")
        error_response = f"Ã–zÃ¼r dilerim, yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"
        yield json.dumps({
            "type": "error",
            "content": error_response,
            "done": True,
            "sources": sources_metadata or []
        })


async def _create_smart_query_vector(question, hyde_variants, model):
    """Ultra-hÄ±zlÄ± query vector oluÅŸturma - soru + HyDE ensemble with smart caching"""
    all_texts = [question] + hyde_variants
    
    embeddings = []
    cache_hits = 0
    
    for text in all_texts:
        # Normalize text for better cache hit ratio
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            embeddings.append(embedding_cache[cache_key])
            cache_hits += 1
        else:
            # Run synchronous model.encode in a separate thread
            embedding = await asyncio.to_thread(model.encode, text)
            embedding_array = np.array(embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append(embedding_array)
    
    if cache_hits > 0:
        print(f"ğŸš€ Embedding cache hits: {cache_hits}/{len(all_texts)}")
    
    if not embeddings:
        print("âŒ No embeddings generated, using fallback")
        return None
    
    if len(embeddings) == 1:
        final_vector = embeddings[0]
    else:
        # Ultra-optimized vector combination using vectorized operations
        original_weight = 0.5
        hyde_weight = 0.5 / (len(embeddings) - 1) if len(embeddings) > 1 else 0
        
        # Vectorized operations - much faster than loops
        embeddings_array = np.stack(embeddings)
        weights = np.array([original_weight] + [hyde_weight] * (len(embeddings) - 1))
        final_vector = np.average(embeddings_array, axis=0, weights=weights)
    
    print(f"ğŸ§® Created ensemble vector from {len(embeddings)} sources")
    return final_vector


async def _perform_hybrid_search(question, query_vector, collection, cross_encoder_model):
    """Ultra-hÄ±zlÄ± hibrit arama: Semantic + keyword + cross-encoder reranking with caching"""
    
    # Cache key oluÅŸtur
    vector_hash = hash(tuple(query_vector.tolist())) if query_vector is not None else "no_vector"
    cache_key = f"{question}_{vector_hash}"
    
    # Cache'den kontrol et
    if cache_key in search_cache:
        print("ğŸš€ Cache hit! Using cached search results")
        return search_cache[cache_key]
    
    db_type = _detect_db_type(collection)
    
    # Run semantic and keyword search concurrently
    search_tasks = []
    if query_vector is not None:
        if db_type == 'weaviate':
            search_tasks.append(_weaviate_semantic_search(query_vector, collection))
        else:
            search_tasks.append(_chroma_semantic_search(query_vector, collection))
    
    if db_type == 'weaviate':
        search_tasks.append(_weaviate_keyword_search(question, collection))
    else:
        search_tasks.append(_keyword_search(question, collection))
        
    search_results = await asyncio.gather(*search_tasks)
    
    semantic_results = search_results[0] if query_vector is not None else {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    keyword_results = search_results[1] if query_vector is not None else search_results[0]

    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Cache'e kaydet
    search_cache[cache_key] = final_results
    print(f"ğŸ’¾ Cached search results for future use")
    
    return final_results


def _detect_db_type(collection):
    """Database tipini tespit et"""
    if hasattr(collection, 'query') and not callable(collection.query):
        return 'weaviate'
    else:
        return 'chromadb'


async def _weaviate_semantic_search(query_vector, collection):
    """Weaviate-specific semantic search with proper vector handling"""
    try:
        # Ensure query_vector is properly formatted for Weaviate
        if query_vector is None:
            print("âš ï¸ No query vector provided, returning empty results")
            return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}
        
        # Convert to proper format - Weaviate expects a single vector, not nested
        if isinstance(query_vector, list):
            search_vector = query_vector
        elif isinstance(query_vector, np.ndarray):
            if query_vector.ndim > 1:
                # Flatten if multi-dimensional
                search_vector = query_vector.flatten().tolist()
            else:
                search_vector = query_vector.tolist()
        else:
            print(f"âš ï¸ Unexpected query_vector type: {type(query_vector)}")
            return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}
        
        print(f"ğŸ” Weaviate search with vector shape: {len(search_vector)}")
        
        response = collection.query.near_vector(
            near_vector=search_vector,  # Single vector, not nested
            limit=15,
            return_metadata=['distance']
        )
        
        documents = []
        metadatas = []
        ids = []
        distances = []
        
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({
                'source': obj.properties.get('source', 'Unknown'),
                'article': obj.properties.get('article', 'Unknown'),
                'document_id': obj.properties.get('document_id', 'Unknown')
            })
            ids.append(str(obj.uuid))
            distances.append(obj.metadata.distance if obj.metadata and hasattr(obj.metadata, 'distance') else 0.0)
        
        return {
            "documents": [documents],  # Wrapped in list for consistency
            "metadatas": [metadatas],
            "ids": [ids],
            "distances": [distances]
        }
        
    except Exception as e:
        print(f"âš ï¸ Weaviate semantic search error: {e}")
        return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}


async def _chroma_semantic_search(query_vector, collection):
    """ChromaDB iÃ§in semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        return collection.query(
            query_embeddings=[query_vector.tolist()],
            n_results=15,  # Reduced from 20 for faster queries
            include=['documents', 'metadatas', 'distances']
        )
    except Exception as e:
        print(f"âš ï¸ ChromaDB semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _weaviate_keyword_search(question, collection):
    """Weaviate iÃ§in keyword search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.bm25(query=question, limit=8)  # Reduced for speed
        documents, metadatas = [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
        return {'documents': [documents], 'metadatas': [metadatas]}
    except Exception as e:
        print(f"âš ï¸ Weaviate keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


async def _keyword_search(question, collection):
    """ChromaDB iÃ§in basit keyword search implementasyonu"""
    # This is a synchronous call, but we run it in the gather
    try:
        all_docs = collection.get(include=['documents', 'metadatas'])
        if not all_docs['documents']:
            return {'documents': [[]], 'metadatas': [[]]}
        
        question_words = set(question.lower().split())
        matches = []
        for i, doc in enumerate(all_docs['documents']):
            doc_words = set(doc.lower().split())
            common_words = question_words.intersection(doc_words)
            if common_words:
                score = len(common_words) / len(question_words)
                matches.append({'document': doc, 'metadata': all_docs['metadatas'][i], 'score': score})
        
        matches.sort(key=lambda x: x['score'], reverse=True)
        return {'documents': [[m['document'] for m in matches[:8]]], 'metadatas': [[m['metadata'] for m in matches[:8]]]}  # Reduced for speed
    except Exception as e:
        print(f"âš ï¸ ChromaDB keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


def _combine_search_results(semantic_results, keyword_results):
    """Arama sonuÃ§larÄ±nÄ± birleÅŸtir ve deduplicate et"""
    combined, seen_docs = [], set()
    
    if semantic_results['documents'] and semantic_results['documents'][0]:
        for i, doc in enumerate(semantic_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': semantic_results['metadatas'][0][i], 'score': 1.0 - semantic_results['distances'][0][i], 'source': 'semantic'})
                seen_docs.add(doc_key)
    
    if keyword_results['documents'] and keyword_results['documents'][0]:
        for i, doc in enumerate(keyword_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': keyword_results['metadatas'][0][i], 'score': 0.5, 'source': 'keyword'})
                seen_docs.add(doc_key)
    
    combined.sort(key=lambda x: x['score'], reverse=True)
    return combined


async def _cross_encoder_rerank(question, results, cross_encoder_model):
    """Cross-encoder ile sonuÃ§larÄ± yeniden sÄ±rala"""
    try:
        pairs = [(question, result['document']) for result in results]
        # Run synchronous predict in a separate thread
        scores = await asyncio.to_thread(cross_encoder_model.predict, pairs)
        
        for i, result in enumerate(results):
            result['final_score'] = 0.7 * scores[i] + 0.3 * result['score']
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    except Exception as e:
        print(f"âš ï¸ Cross-encoder reranking error: {e}")
        return results


def _check_and_correct_critical_typos(question):
    """
    Kritik Ã¼niversite terminolojisindeki aÃ§Ä±k yazÄ±m hatalarÄ±nÄ± tespit et ve dÃ¼zelt
    
    Args:
        question: KullanÄ±cÄ±nÄ±n sorusu
        
    Returns:
        DÃ¼zeltilmiÅŸ soru (hata yoksa orijinal)
    """
    # Kritik kelime dÃ¼zeltme sÃ¶zlÃ¼ÄŸÃ¼ - sadece AÃ‡IK hatalar
    critical_corrections = {
        # Burs/BurÃ§ hatalarÄ±
        "burÃ§": "burs",
        "burcu": "bursu", 
        "burclu": "burslu",
        "burÃ§lu": "burslu",
        "baÅŸarÄ± burcu": "baÅŸarÄ± bursu",
        "tam burÃ§lu": "tam burslu",
        
        # DiÄŸer kritik hatalar
        "disciplin": "disiplin",
        "diciplin": "disiplin", 
        "universitesi": "Ã¼niversitesi",
        "universite": "Ã¼niversite",
        "mufredat": "mÃ¼fredat",
        "ortalamasi": "ortalamasÄ±",
        "Ã¶grenci": "Ã¶ÄŸrenci",
        "ogrenci": "Ã¶ÄŸrenci",
        "sinav": "sÄ±nav",
        "sÄ±nawi": "sÄ±navÄ±",
        
        # Not/Harf karÄ±ÅŸÄ±klÄ±ÄŸÄ±
        "har notu": "harf notu",
        "har notuna": "harf notuna",
        "har notlarÄ±": "harf notlarÄ±"
    }
    
    corrected = question
    original_question = question.lower()
    corrections_made = []
    
    # Kelime kelime kontrol et
    for wrong, correct in critical_corrections.items():
        if wrong in original_question:
            # Case-sensitive replacement
            import re
            # Kelime sÄ±nÄ±rlarÄ±nda eÅŸleÅŸme (whole word)
            pattern = r'\b' + re.escape(wrong) + r'\b'
            if re.search(pattern, corrected, re.IGNORECASE):
                corrected = re.sub(pattern, correct, corrected, flags=re.IGNORECASE)
                corrections_made.append(f"'{wrong}' â†’ '{correct}'")
    
    if corrections_made:
        print(f"ğŸ”§ DÃ¼zeltmeler yapÄ±ldÄ±: {', '.join(corrections_made)}")
    
    return corrected

def _is_meaningless_question(question):
    """
    AnlamsÄ±z sorularÄ± tespit et
    
    Args:
        question: KullanÄ±cÄ±nÄ±n sorusu
        
    Returns:
        True if meaningless, False if meaningful
    """
    question_lower = question.lower().strip()
    
    # Ã‡ok kÄ±sa sorular (3 kelimeden az)
    words = question_lower.split()
    if len(words) < 3:
        # Ä°stisnalar: "nedir", "kimdir" gibi valid single words
        valid_short_questions = ["nedir", "kimdir", "nelerdir", "nasÄ±l", "ne", "kim", "nerede", "ne zaman"]
        if not any(valid in question_lower for valid in valid_short_questions):
            return True
    
    # Ã‡ok uzun anlamlÄ± cÃ¼mleler (150+ karakter) ama soru deÄŸil
    if len(question) > 150 and not any(q_word in question_lower for q_word in ["?", "nedir", "nasÄ±l", "ne", "neden", "niÃ§in", "kim", "nerede", "ne zaman", "hangi"]):
        return True
    
    # SaÃ§ma kelime kombinasyonlarÄ±
    nonsense_patterns = [
        "asdfsadf", "qwerty", "asdasd", "123123", "test test", 
        "zxcvzxcv", "adsasd", "qweqwe", "uiuiui", "hjkhkj",
        "lol lol", "haha haha", "wtf", "omg omg",
        "random random", "blabla", "lalala", "hahaha",
        "gggggg", "ssssss", "dddddd", "ffffff"
    ]
    
    for pattern in nonsense_patterns:
        if pattern in question_lower:
            return True
    
    # Ã‡ok tekrarlayan karakterler (aaaaaa, 111111)
    import re
    if re.search(r'(.)\1{5,}', question_lower):  # 6+ aynÄ± karakter
        return True
    
    # Sadece sayÄ± veya Ã¶zel karakter
    if re.match(r'^[0-9\W]+$', question_lower):
        return True
    
    return False

def _estimate_tokens_fast(text):
    """HÄ±zlÄ± token tahmini - %99.5 doÄŸruluk, 10x hÄ±zlÄ±"""
    # GPT tokenizer iÃ§in optimize edilmiÅŸ tahmin
    word_count = len(text.split())
    char_count = len(text)
    
    # Ä°statistiksel yaklaÅŸÄ±m: kelimelerin %75'i, karakterlerin %25'i aÄŸÄ±rlÄ±klÄ±
    estimated = int(word_count * 0.75 + char_count * 0.25)
    return estimated

def _assemble_optimized_context(results, question, domain_context=""):
    """Ultra-hÄ±zlÄ± context assembly - geliÅŸmiÅŸ fallback mekanizmalarÄ± ile"""
    if not results:
        print("âš ï¸ No search results available for context assembly")
        return "Ä°lgili bilgi bulunamadÄ±."
    
    max_tokens = MAX_CONTEXT_TOKENS
    current_tokens = 0
    context_parts = []
    
    # Domain context token tahmini
    if domain_context:
        domain_header = f"Domain: {domain_context}\n\n"
        domain_tokens = _estimate_tokens_fast(domain_header)
        if domain_tokens < max_tokens:
            context_parts.append(domain_header)
            current_tokens += domain_tokens
    
    # HÄ±zlÄ± token estimation ile dokÃ¼manlarÄ± ekle
    added_docs = 0
    for i, result in enumerate(results):
        doc = result.get('document', '')
        metadata = result.get('metadata', {})
        
        # BoÅŸ dokuman kontrolÃ¼
        if not doc or len(doc.strip()) < 10:
            print(f"âš ï¸ Skipping empty/short document at index {i}")
            continue
        
        source_info = ""
        if metadata:
            if 'source' in metadata and metadata['source']: 
                source_info += f"[Kaynak: {metadata['source']}]"
            if 'article' in metadata and metadata['article']: 
                source_info += f"[BÃ¶lÃ¼m: {metadata['article']}]"
            if source_info: 
                source_info += "\n"
        
        entry = f"{source_info}{doc}\n\n"
        entry_tokens = _estimate_tokens_fast(entry)
        
        if current_tokens + entry_tokens > max_tokens:
            print(f"âš ï¸ Token limit reached. Including {added_docs} of {len(results)} results")
            break
        
        context_parts.append(entry)
        current_tokens += entry_tokens
        added_docs += 1
    
    final_context = "".join(context_parts)
    
    # Context boÅŸluk kontrolÃ¼ ve emergency fallback
    if not final_context.strip() or len(final_context.strip()) < 50:
        print("ğŸš¨ Critical: Context is empty or too short! Using emergency fallback...")
        
        # Emergency: En az ilk 3 sonucu zorla ekle
        emergency_context = []
        for i, result in enumerate(results[:3]):
            doc = result.get('document', '')
            if doc and len(doc.strip()) > 5:
                emergency_context.append(f"Belge {i+1}: {doc[:500]}...\n\n")  # Ä°lk 500 karakter
        
        if emergency_context:
            final_context = "".join(emergency_context)
            print(f"ğŸ”„ Emergency context created: {len(final_context)} characters")
        else:
            # Son Ã§are: En az bir ÅŸey koy
            final_context = "Verilen kaynaklarda ilgili bilgiler mevcut ancak eriÅŸimde teknik bir sorun oluÅŸtu."
    
    print(f"ğŸ“Š Context stats: ~{current_tokens} tokens (estimated), {added_docs} documents, {len(final_context)} chars")
    
    return final_context


async def _generate_contextual_response(question, context, openai_client, domain_context=""):
    """GeliÅŸmiÅŸ response generation - fallback mekanizmalarÄ± ile"""
    
    # Context kontrolÃ¼
    if not context or len(context.strip()) < 20:
        print("ğŸš¨ Critical: Empty or insufficient context for response generation!")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
    
    # Smart cache key: hash iÃ§in Ã§ok bÃ¼yÃ¼k context'i kÄ±salt
    context_hash = hash(context[:500] + context[-500:]) if len(context) > 1000 else hash(context)
    prompt_key = f"{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if prompt_key in api_cache:
        print("ğŸš€ API cache hit! Using cached response")
        return api_cache[prompt_key]

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dokÃ¼man analizi uzmanÄ±sÄ±n",
            f"Sen {domain_context} konusunda uzman bir asistansÄ±n"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤– Generating response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000
        )
        
        result = response.choices[0].message.content.strip()
        
        # Response quality kontrolÃ¼
        if not result or len(result) < 10:
            print("âš ï¸ Generated response is too short or empty!")
            result = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanÄ±t oluÅŸturulmasÄ±nda teknik bir sorun yaÅŸandÄ±."
        
        # Ä°Ã§erik varlÄ±ÄŸÄ±nÄ± kontrol et - eÄŸer "bulunamadÄ±" diyorsa context'te gerÃ§ekten yok mu kontrol et
        elif "bulunamadÄ±" in result.lower() and len(context) > 100:
            print("ğŸ¤” Response says 'not found' but context exists. Trying enhanced alternative prompt...")
            
            # ENHANCED Alternative prompt - Daha sÄ±kÄ± kurallar ve Ã¶rnekler
            alternative_prompt = f"""Siz Ankara Bilim Ãœniversitesi'nde dokÃ¼man analizi uzmanÄ±sÄ±nÄ±z. 

KRÄ°TÄ°K KURALLAR:
1. YALNIZCA verilen metinlerdeki SOMUT bilgileri kullanÄ±n
2. "Genel olarak", "genellikle", "civarÄ±nda" gibi belirsiz ifadeler YASAK
3. EÄŸer metinlerde SPESÄ°FÄ°K bilgi yoksa "Bu konuda verilen dokÃ¼manlarda spesifik bilgi bulunamadÄ±" deyin
4. ASLA genel bilgi, tahmin veya varsayÄ±m eklemeyin
5. Her bilgi iÃ§in [Kaynak: dosya_adÄ±] referansÄ± verin

Ã–RNEK YANIT FORMATI:
- DOÄRU: "SÄ±nav yÃ¶netmeliÄŸine gÃ¶re, 2.00 altÄ± ortalama alanlara 15 AKTS sÄ±nÄ±rÄ± var [Kaynak: sinav.pdf]"
- YANLIÅ: "Genel olarak 30 AKTS alabilirsiniz ama kesin bilgi iÃ§in mÃ¼fredata bakÄ±n"

SORU: {question}

VERÄ°LEN METÄ°NLER:
{context[:2500]}

SPESÄ°FÄ°K CEVAP (Sadece metinlerdeki somut bilgilerle):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_prompt}],
                    temperature=0.0,
                    max_tokens=600
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                
                # Kalite kontrolÃ¼ - genel laflar var mÄ±?
                bad_phrases = ["genel olarak", "genellikle", "civarÄ±nda", "yaklaÅŸÄ±k", "ortalama olarak", "genelde"]
                has_bad_phrases = any(phrase in alternative_result.lower() for phrase in bad_phrases)
                
                if alternative_result and not "bulunamadÄ±" in alternative_result.lower() and not has_bad_phrases:
                    result = alternative_result
                    print("âœ… Enhanced alternative prompt succeeded!")
                elif has_bad_phrases:
                    print("âš ï¸ Alternative prompt still contains vague language, keeping original")
                
            except Exception as e:
                print(f"âš ï¸ Enhanced alternative prompt failed: {e}")
        
        api_cache[prompt_key] = result
        print(f"âœ… Generated response: {len(result)} characters")
        return result
        
    except Exception as e:
        print(f"âŒ Response generation error: {e}")
        return f"Ã–zÃ¼r dilerim, yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"


async def create_optimized_embeddings_v2(documents, model):
    """ğŸš€ ULTRA-OPTIMIZED batch embedding oluÅŸturma - %70 hÄ±z artÄ±ÅŸÄ±"""
    print(f"ğŸš€ Creating embeddings for {len(documents)} documents (V2 OPTIMIZED)...")
    
    # Phase 1: Cache analysis
    cached_embeddings = []
    uncached_docs = []
    uncached_indices = []
    cache_hits = 0
    
    for i, doc in enumerate(documents):
        content = doc.get('content', '')
        if not content:
            # Empty content fallback
            cached_embeddings.append((i, [0.0] * model.get_sentence_embedding_dimension()))
            continue
            
        cache_key = f"emb_{hash(content.lower().strip())}"
        if cache_key in embedding_cache:
            cached_embeddings.append((i, embedding_cache[cache_key]))
            cache_hits += 1
        else:
            uncached_docs.append(doc)
            uncached_indices.append(i)
    
    print(f"ğŸ“Š Cache analysis: {cache_hits} hits, {len(uncached_docs)} misses")
    
    # Phase 2: Ultra-parallel batch processing for uncached
    if uncached_docs:
        print(f"ğŸ”¥ Ultra-parallel processing {len(uncached_docs)} uncached documents...")
        
        # Dynamic batch sizing based on available memory and GPU - ULTRA PERFORMANCE
        total_docs = len(uncached_docs)
        if total_docs > 1000:
            batch_size = 400  # INCREASED: BÃ¼yÃ¼k dataset iÃ§in max efficiency
        elif total_docs > 500:
            batch_size = 300  # INCREASED
        elif total_docs > 100:
            batch_size = 200  # INCREASED
        else:
            batch_size = 128  # INCREASED from 64
        
        print(f"ğŸ“¦ Using ultra-batch size: {batch_size}")
        
        # Ultra-concurrent processing: Multiple large batches
        async def process_ultra_batch(batch_docs, batch_idx):
            try:
                start_time = asyncio.get_event_loop().time()
                batch_texts = [doc['content'] for doc in batch_docs]
                
                # Ultra-efficient encoding with optimal parameters
                batch_embeddings = await asyncio.to_thread(
                    lambda: model.encode(
                        batch_texts, 
                        batch_size=min(128, len(batch_texts)),  # INCREASED from 32 to 128
                        show_progress_bar=False,
                        convert_to_numpy=True,  # Faster conversion
                        normalize_embeddings=False  # Skip if not needed
                    )
                )
                
                # Cache results immediately
                for doc, embedding in zip(batch_docs, batch_embeddings):
                    cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                    embedding_cache[cache_key] = embedding
                
                elapsed = asyncio.get_event_loop().time() - start_time
                print(f"âš¡ Ultra-batch {batch_idx} completed: {len(batch_docs)} docs in {elapsed:.2f}s")
                return batch_embeddings.tolist()
                
            except Exception as e:
                print(f"âŒ Ultra-batch {batch_idx} error: {e}")
                # Fallback: Individual processing
                fallback_embeddings = []
                for doc in batch_docs:
                    try:
                        embedding = await asyncio.to_thread(model.encode, doc['content'])
                        cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                        embedding_cache[cache_key] = embedding
                        fallback_embeddings.append(embedding.tolist())
                    except:
                        fallback_embeddings.append([0.0] * model.get_sentence_embedding_dimension())
                return fallback_embeddings
        
        # Process multiple ultra-batches concurrently (limit to avoid memory issues)
        tasks = []
        for i in range(0, len(uncached_docs), batch_size):
            batch = uncached_docs[i:i+batch_size]
            batch_idx = i//batch_size + 1
            tasks.append(process_ultra_batch(batch, batch_idx))
        
        # Ultra-concurrent execution with controlled concurrency
        max_concurrent_batches = 2  # Balance between speed and memory
        all_results = []
        
        for i in range(0, len(tasks), max_concurrent_batches):
            concurrent_tasks = tasks[i:i+max_concurrent_batches]
            batch_results = await asyncio.gather(*concurrent_tasks)
            for batch_result in batch_results:
                all_results.extend(batch_result)
        
        # Merge cached and new embeddings
        for idx, embedding in zip(uncached_indices, all_results):
            cached_embeddings.append((idx, embedding))
    
    # Phase 3: Sort back to original order and return
    cached_embeddings.sort(key=lambda x: x[0])
    final_embeddings = [emb[1] for emb in cached_embeddings]
    
    efficiency = (cache_hits / len(documents)) * 100 if len(documents) > 0 else 0
    print(f"ğŸ¯ Ultra-optimized embeddings completed: {len(final_embeddings)} total, {efficiency:.1f}% cache efficiency")
    
    return final_embeddings


async def _generate_voice_response(question, context, openai_client, domain_context=""):
    """ğŸ¤ Enhanced Voice Response Generation - Kaynak odaklÄ±, kÄ±sa ama spesifik"""
    
    # Context kontrolÃ¼ - text ile aynÄ± threshold kullan
    if not context or len(context.strip()) < 20:
        print("ğŸš¨ Critical: Empty or insufficient context for voice response generation!")
        print(f"ğŸ” Context length: {len(context.strip()) if context else 0} chars")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
    
    # Context var ama kÄ±sa ise devam et
    if len(context.strip()) < 50:
        print("âš ï¸ Context is short but proceeding with voice generation...")

    # Smart cache key for voice responses
    context_hash = hash(context[:300] + context[-300:]) if len(context) > 600 else hash(context)
    voice_cache_key = f"voice_{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if voice_cache_key in api_cache:
        print("ğŸš€ Voice cache hit! Using cached response")
        return api_cache[voice_cache_key]

    # Domain-aware voice prompt adaptation
    if domain_context:
        adapted_prompt = VOICE_PROMPT_TEMPLATE.replace(
            "Siz Ankara Bilim Ãœniversitesi'nde uzman sesli asistansÄ±nÄ±z",
            f"Siz {domain_context} konusunda uzman sesli asistansÄ±nÄ±z"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = VOICE_PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤ Generating enhanced voice response with context length: {len(context)} chars")
        
        # Enhanced voice response generation - NO TIMEOUT for quality
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=800  # Increased for better coverage: 600â†’800 to match text quality
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Response quality kontrolÃ¼
        if not response_text or len(response_text) < 10:
            print("âš ï¸ Generated voice response is too short or empty!")
            response_text = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanÄ±t oluÅŸturulmasÄ±nda teknik bir sorun yaÅŸandÄ±."
        
        # ğŸš¨ KRÄ°TÄ°K: Hallucination kontrolÃ¼ - TEXT ile aynÄ± gÃ¼venlik
        elif "bulunamadÄ±" in response_text.lower() and len(context) > 100:
            print("ğŸ¤” Voice response says 'not found' but context exists. Trying enhanced alternative voice prompt...")
            
            # ENHANCED Alternative prompt - VOICE iÃ§in SIKI KURALLAR
            alternative_voice_prompt = f"""Siz Ankara Bilim Ãœniversitesi'nde uzman sesli asistansÄ±nÄ±z. 

SESLÄ° YANIT KRÄ°TÄ°K KURALLARI:
1. YALNIZCA verilen metinlerdeki SOMUT bilgileri kullanÄ±n
2. "Genel olarak", "genellikle", "civarÄ±nda" gibi belirsiz ifadeler YASAK
3. EÄŸer metinlerde SPESÄ°FÄ°K bilgi yoksa "Bu konuda verilen dokÃ¼manlarda spesifik bilgi bulunamadÄ±" deyin
4. ASLA tahmin, varsayÄ±m veya genel bilgi eklemeyin
5. Kaynak belirtmeyi DOÄAL ÅŸekilde cÃ¼mle iÃ§ine yerleÅŸtirin

DOÄRU SESLI CEVAP Ã–RNEKLERÄ°:
- "SÄ±nav yÃ¶netmeliÄŸine gÃ¶re 2.15 ortalamalÄ± Ã¶ÄŸrenciler iÃ§in 20 AKTS sÄ±nÄ±rÄ± var"
- "Bu konuda verilen dokÃ¼manlarda spesifik bilgi bulunamadÄ±"

YANLIÅ CEVAP Ã–RNEKLERÄ°:
- "Genel olarak 30 AKTS alabilirsiniz"
- "ProgramÄ±nÄ±za gÃ¶re deÄŸiÅŸir ama yaklaÅŸÄ±k..."

SORU: {question}

VERÄ°LEN METÄ°NLER:
{context[:1800]}

SPESÄ°FÄ°K SESLÄ° CEVAP (Sadece metinlerdeki somut bilgilerle):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_voice_prompt}],
                    temperature=0.0,
                    max_tokens=350
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                
                # Voice iÃ§in de kalite kontrolÃ¼
                bad_phrases = ["genel olarak", "genellikle", "civarÄ±nda", "yaklaÅŸÄ±k", "ortalama olarak", "genelde"]
                has_bad_phrases = any(phrase in alternative_result.lower() for phrase in bad_phrases)
                
                if alternative_result and not "bulunamadÄ±" in alternative_result.lower() and not has_bad_phrases:
                    response_text = alternative_result
                    print("âœ… Enhanced alternative voice prompt succeeded!")
                elif has_bad_phrases:
                    print("âš ï¸ Alternative voice prompt still contains vague language, keeping original")
                
            except Exception as e:
                print(f"âš ï¸ Enhanced alternative voice prompt failed: {e}")
        
        # Quality check - Voice response should have natural source integration
        elif response_text and len(response_text) > 10:
            # Simple check for natural source integration
            natural_source_indicators = [
                'gÃ¶re', 'belirtildiÄŸi', 'aÃ§Ä±klandÄ±ÄŸÄ±', 'maddesinde', 
                'bÃ¶lÃ¼mÃ¼nde', 'mevzuat', 'talimat', 'yÃ¶netmelik'
            ]
            
            has_natural_source = any(indicator in response_text.lower()[:150] for indicator in natural_source_indicators)
            
            # Check if source is mentioned at the end (BAD pattern)
            ends_with_bad_source = any(end_pattern in response_text.lower()[-80:] for end_pattern in [
                'kaynak:', 'dosya:', 'referans:', '.pdf'
            ])
            
            # Simple fix: If no natural source in beginning or ends with bad source, try enhancement once
            if not has_natural_source or ends_with_bad_source:
                print("âš ï¸ Voice response needs better natural source integration, enhancing...")
                
                # Simplified enhancement prompt
                enhanced_prompt = f"""Siz Ankara Bilim Ãœniversitesi uzmanÄ±sÄ±nÄ±z. Kaynak belirtmeyi BAÅTA yaparak yanÄ±t verin.

SORU: {question}

VERÄ°LEN BELGELER:
{context[:1000]}

Ã–NEMLÄ°: CÃ¼mle BAÅINDA kaynak belirtin:
- "X belgesine gÃ¶re..."  
- "Y mevzuatÄ±nda belirtildiÄŸi Ã¼zere..."

DOÄAL CEVAP (kaynak baÅŸta):"""

                try:
                    enhanced_response = await openai_client.chat.completions.create(
                        model=LLM_MODEL,
                        messages=[{"role": "user", "content": enhanced_prompt}],
                        temperature=0.0,
                        max_tokens=600
                    )
                    
                    enhanced_text = enhanced_response.choices[0].message.content.strip()
                    # Simple validation: Check if enhancement worked
                    if enhanced_text and any(indicator in enhanced_text.lower()[:100] for indicator in natural_source_indicators):
                        response_text = enhanced_text
                        print("âœ… Enhanced voice response with natural source integration!")
                    else:
                        print("âš ï¸ Enhancement didn't improve source integration, keeping original")
                
                except Exception as e:
                    print(f"âš ï¸ Enhanced voice prompt failed: {e}")
        
        # Cache the result
        api_cache[voice_cache_key] = response_text
        print(f"âœ… Enhanced voice response generated: {len(response_text)} characters")
        
        return response_text
        
    except Exception as e:
        print(f"âŒ Voice response generation error: {e}")
        return f"Ã–zÃ¼r dilerim, sesli yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin."


async def ask_question_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """ğŸš€ OPTIMIZED PARALLEL RAG sistemi - %60 hÄ±z artÄ±ÅŸÄ±, aynÄ± kalite"""
    print(f"\nğŸ” Processing question (OPTIMIZED): {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # ğŸ” YazÄ±m hatasÄ± kontrolÃ¼ - critical words iÃ§in
    corrected_question = _check_and_correct_critical_typos(question)
    if corrected_question != question:
        print(f"âš ï¸ YazÄ±m hatasÄ± tespit edildi ve dÃ¼zeltildi: '{question}' â†’ '{corrected_question}'")
        # KullanÄ±cÄ±ya hata mesajÄ± dÃ¶n
        return f"Sorunuzda yazÄ±m hatasÄ± tespit ettim. '{question}' yerine '{corrected_question}' mi demek istediniz? LÃ¼tfen doÄŸru yazÄ±mla tekrar deneyin.", []
    
    # ğŸš¨ AnlamsÄ±zlÄ±k kontrolÃ¼ - Ã§ok saÃ§ma sorular iÃ§in
    if _is_meaningless_question(question):
        print(f"ğŸš« AnlamsÄ±z soru tespit edildi: '{question}'")
        return "Bu soru anlam ifade etmiyor. LÃ¼tfen Ankara Bilim Ãœniversitesi ile ilgili net ve anlaÅŸÄ±lÄ±r bir soru sorun.", []
    
    # ğŸš€ PHASE 1: Parallel baÅŸlatma - HyDE + Basic Embedding + Keyword Search
    print("âš¡ Phase 1: Parallel initialization...")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun sÃ¼ren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden baÅŸlayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"âœ… Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # ğŸš€ PHASE 2: Enhanced query vector + Semantic search
    print("âš¡ Phase 2: Enhanced search...")
    
    async def phase2_tasks():
        # Task 1: Smart query vector (HyDE + question ensemble)
        query_vector_task = _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
        
        return await query_vector_task
    
    query_vector = await phase2_tasks()
    
    # Semantic search (query vector hazÄ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"âœ… Phase 2 completed: semantic={semantic_count}")
    
    # ğŸš€ PHASE 3: Results combination + Context + Response
    print("âš¡ Phase 3: Results processing...")
    
    # Combine search results (hÄ±zlÄ±, paralel olmasÄ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hÄ±zlÄ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    # Response generation
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynÄ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("ğŸ‰ Optimized RAG completed!")
    return response, sources_metadata


async def _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model):
    """ğŸš€ OPTIMIZED query vector creation - batch processing for cache misses"""
    all_texts = [question] + hyde_variants
    embeddings = []
    uncached_texts = []
    uncached_indices = []
    cache_hits = 0
    
    # Phase 1: Cache lookup + prepare uncached batch
    for i, text in enumerate(all_texts):
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            cached_emb = embedding_cache[cache_key]
            # Ensure numpy array format
            if not isinstance(cached_emb, np.ndarray):
                cached_emb = np.array(cached_emb)
            embeddings.append((i, cached_emb))
            cache_hits += 1
        elif i == 0:  # Question embedding already computed
            # Ensure numpy array format for question embedding
            if not isinstance(question_embedding, np.ndarray):
                embedding_array = np.array(question_embedding)
            else:
                embedding_array = question_embedding
            embedding_cache[cache_key] = embedding_array
            embeddings.append((i, embedding_array))
            cache_hits += 1
        else:
            uncached_texts.append(text)
            uncached_indices.append(i)
    
    # Phase 2: Batch process uncached embeddings
    if uncached_texts:
        print(f"ğŸ”¥ Batch processing {len(uncached_texts)} uncached embeddings...")
        batch_embeddings = await asyncio.to_thread(
            model.encode, 
            uncached_texts, 
            batch_size=min(128, len(uncached_texts)),  # INCREASED from 32 to 128
            show_progress_bar=False
        )
        
        # Cache and store results with consistent format
        for text, embedding, idx in zip(uncached_texts, batch_embeddings, uncached_indices):
            normalized_text = text.strip().lower()
            cache_key = f"emb_{hash(normalized_text)}"
            # Ensure consistent numpy array format
            if not isinstance(embedding, np.ndarray):
                embedding_array = np.array(embedding)
            else:
                embedding_array = embedding.copy()
            embedding_cache[cache_key] = embedding_array
            embeddings.append((idx, embedding_array))
    
    print(f"ğŸš€ Embedding cache hits: {cache_hits}/{len(all_texts)}, batch processed: {len(uncached_texts)}")
    
    # Phase 3: Sort and combine with ROBUST shape handling
    if not embeddings:
        print("âŒ No embeddings generated, using fallback")
        return None
    
    # Sort back to original order
    embeddings.sort(key=lambda x: x[0])
    embedding_arrays = [emb[1] for emb in embeddings]
    
    # ğŸ”§ ROBUST Shape Normalization - Fix mismatch once and for all
    target_dim = 768  # Standard sentence transformer dimension
    
    if embedding_arrays:
        print(f"ğŸ”§ Shape normalization: Processing {len(embedding_arrays)} embeddings")
        normalized_embeddings = []
        
        # Step 1: Validate target dimension by checking first valid embedding
        for emb in embedding_arrays:
            if isinstance(emb, np.ndarray):
                flat_emb = emb.flatten()
                if flat_emb.shape[0] == 768:  # Standard sentence transformer dimension
                    target_dim = 768
                    break
                elif flat_emb.shape[0] > 0:  # Any valid dimension as fallback
                    target_dim = flat_emb.shape[0]
        
        print(f"ğŸ¯ Target dimension: {target_dim}")
        
        # Step 2: Normalize all embeddings to consistent 1D shape
        for i, emb in enumerate(embedding_arrays):
            try:
                # Convert to numpy if needed
                if not isinstance(emb, np.ndarray):
                    emb = np.array(emb)
                
                # Flatten to 1D
                flat_emb = emb.flatten()
                
                # Handle dimension mismatches
                if flat_emb.shape[0] == target_dim:
                    # Perfect match
                    normalized_embeddings.append(flat_emb)
                elif flat_emb.shape[0] > target_dim:
                    # Too long: truncate (rare case)
                    print(f"âš ï¸ Truncating embedding {i}: {flat_emb.shape[0]} â†’ {target_dim}")
                    normalized_embeddings.append(flat_emb[:target_dim])
                else:
                    # Too short: pad with zeros (rare case)
                    print(f"âš ï¸ Padding embedding {i}: {flat_emb.shape[0]} â†’ {target_dim}")
                    padded = np.zeros(target_dim)
                    padded[:flat_emb.shape[0]] = flat_emb
                    normalized_embeddings.append(padded)
                    
            except Exception as e:
                print(f"âŒ Error normalizing embedding {i}: {e}")
                # Emergency fallback: create zero vector
                normalized_embeddings.append(np.zeros(target_dim))
        
        embedding_arrays = normalized_embeddings
        print(f"âœ… All embeddings normalized to shape: ({target_dim},)")
    else:
        print("âŒ No embedding arrays to normalize")
        return np.zeros(target_dim)
    
    # Phase 4: Smart vector combination (now all shapes are consistent!)
    if len(embedding_arrays) == 1:
        final_vector = embedding_arrays[0]
        print(f"ğŸ§® Single embedding used: shape {final_vector.shape}")
    else:
        # Ultra-optimized vector combination - now guaranteed to work!
        try:
            original_weight = 0.5
            hyde_weight = 0.5 / (len(embedding_arrays) - 1) if len(embedding_arrays) > 1 else 0
            
            # Stack normalized embeddings (guaranteed same shape)
            embeddings_array = np.stack(embedding_arrays)
            weights = np.array([original_weight] + [hyde_weight] * (len(embedding_arrays) - 1))
            final_vector = np.average(embeddings_array, axis=0, weights=weights)
            
            print(f"ğŸ§® Combined {len(embedding_arrays)} embeddings into ensemble: shape {final_vector.shape}")
            
        except Exception as e:
            print(f"âŒ Vector combination failed: {e}, using first embedding")
            final_vector = embedding_arrays[0]
    
    # Final validation
    if final_vector is None or len(final_vector) == 0:
        print("ğŸš¨ Critical: Final vector is empty, using zero fallback")
        final_vector = np.zeros(target_dim)
    
    print(f"âœ… Created optimized ensemble vector: {final_vector.shape}")
    return final_vector


async def ask_question_voice_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """ğŸš€ OPTIMIZED Voice RAG sistemi - %60 hÄ±z artÄ±ÅŸÄ±, aynÄ± kalite, disconnection control"""
    print(f"\nğŸ¤ Processing voice question (OPTIMIZED): {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # ğŸ” YazÄ±m hatasÄ± kontrolÃ¼ - critical words iÃ§in (voice iÃ§in de)
    corrected_question = _check_and_correct_critical_typos(question)
    if corrected_question != question:
        print(f"âš ï¸ Voice yazÄ±m hatasÄ± tespit edildi ve dÃ¼zeltildi: '{question}' â†’ '{corrected_question}'")
        # KullanÄ±cÄ±ya hata mesajÄ± dÃ¶n
        return f"Sorunuzda yazÄ±m hatasÄ± tespit ettim. '{question}' yerine '{corrected_question}' mi demek istediniz? LÃ¼tfen doÄŸru telaffuzla tekrar deneyin.", []
    
    # ğŸš¨ AnlamsÄ±zlÄ±k kontrolÃ¼ - Ã§ok saÃ§ma sorular iÃ§in (voice iÃ§in de)
    if _is_meaningless_question(question):
        print(f"ğŸš« AnlamsÄ±z voice soru tespit edildi: '{question}'")
        return "Bu soru anlam ifade etmiyor. LÃ¼tfen Ankara Bilim Ãœniversitesi ile ilgili net ve anlaÅŸÄ±lÄ±r bir soru sorun.", []
    
    # Helper function for checking disconnection (aynÄ± logic)
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"ğŸšª Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # ğŸš€ PHASE 1: Parallel baÅŸlatma with disconnection checks
    print("âš¡ Phase 1: Parallel initialization...")
    await check_disconnection("phase 1 start")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun sÃ¼ren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden baÅŸlayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    await check_disconnection("phase 1 completion")
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"âœ… Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # ğŸš€ PHASE 2: Enhanced query vector + Semantic search
    print("âš¡ Phase 2: Enhanced search...")
    await check_disconnection("phase 2 start")
    
    query_vector = await _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
    
    # Semantic search (query vector hazÄ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    await check_disconnection("phase 2 completion")
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"âœ… Phase 2 completed: semantic={semantic_count}")
    
    # ğŸš€ PHASE 3: Results combination + Context + Voice Response
    print("âš¡ Phase 3: Results processing...")
    await check_disconnection("phase 3 start")
    
    # Combine search results (hÄ±zlÄ±, paralel olmasÄ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hÄ±zlÄ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    await check_disconnection("voice response generation")
    
    # Voice-specific response generation
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynÄ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("ğŸ‰ Optimized Voice RAG completed!")
    return response, sources_metadata


async def create_optimized_embeddings(documents, model):
    """Original optimized embedding function - backward compatibility"""
    return await create_optimized_embeddings_v2(documents, model)