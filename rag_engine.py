import asyncio
import uuid
from typing import List, Dict, Any, Optional
from config import LLM_MODEL, HYDE_LLM_MODEL
import numpy as np
from hyde_generator import generate_hypothetical_answers, generate_multiple_hyde_variants
from config import MAX_CONTEXT_TOKENS, PROMPT_TEMPLATE, VOICE_PROMPT_TEMPLATE
from cachetools import TTLCache
import json

# Ultra-optimized caches with memory management
embedding_cache = TTLCache(maxsize=2000, ttl=900)  # 2K embeddings, 15 min
api_cache = TTLCache(maxsize=300, ttl=600)  # 300 API responses, 10 min  
search_cache = TTLCache(maxsize=500, ttl=450)  # 500 search results, 7.5 min

# Memory optimization: Object pool for frequent operations
_temp_vector_pool = []
_max_pool_size = 50

def _get_temp_vector(size):
    """Get a temporary vector from pool or create new one"""
    if _temp_vector_pool:
        vector = _temp_vector_pool.pop()
        if len(vector) == size:
            vector.fill(0)  # Reset values
            return vector
    return np.zeros(size)

def _return_temp_vector(vector):
    """Return vector to pool for reuse"""
    if len(_temp_vector_pool) < _max_pool_size:
        _temp_vector_pool.append(vector)

async def ask_question(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Genel ama√ßlƒ± RAG sistemi - her t√ºrl√º dok√ºmana uyum saƒülar"""
    print(f"\nüîç Processing question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Adƒ±m 1: Geli≈ümi≈ü HyDE - √áoklu varyant yakla≈üƒ±mƒ±
    print("üìù Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # Adƒ±m 2: Akƒ±llƒ± Query Vector Olu≈üturma
    print("üß† Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # Adƒ±m 3: Hibrit Arama
    print("üîç Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # Adƒ±m 4: Geli≈ümi≈ü Context Assembly
    print("üîß Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Adƒ±m 5: Final Response Generation
    print("‚ú® Step 5: Response generation...")
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_voice(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """Voice-specific RAG sistemi - Aynƒ± RAG engine, sadece voice-optimized response"""
    print(f"\nüé§ Processing voice question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Helper function for checking disconnection
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"üö™ Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # AYNI RAG ENGINE'I KULLAN - ask_question ile aynƒ± logic
    print("üìù Step 1: Universal HyDE generation...")
    await check_disconnection("HyDE generation")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    print("üß† Step 2: Smart query vector creation...")
    await check_disconnection("query vector creation")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    print("üîç Step 3: Hybrid search execution...")
    await check_disconnection("hybrid search")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    print("üîß Step 4: Advanced context assembly...")
    await check_disconnection("context assembly")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # FARK: Sadece voice-specific response generation
    print("üé§ Step 5: Voice-optimized response generation...")
    await check_disconnection("voice response generation")
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_stream(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Streaming version of ask_question - yields responses in real-time"""
    print(f"\nüîç Processing streaming question: {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Adƒ±m 1: Geli≈ümi≈ü HyDE - √áoklu varyant yakla≈üƒ±mƒ±
    print("üìù Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # Adƒ±m 2: Akƒ±llƒ± Query Vector Olu≈üturma
    print("üß† Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # Adƒ±m 3: Hibrit Arama
    print("üîç Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # Adƒ±m 4: Geli≈ümi≈ü Context Assembly
    print("üîß Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    # Adƒ±m 5: Streaming Response Generation
    print("‚ú® Step 5: Streaming response generation...")
    async for chunk in _generate_streaming_response(question, context, openai_client, domain_context, sources_metadata):
        yield chunk


async def _generate_streaming_response(question, context, openai_client, domain_context="", sources_metadata=None):
    """Generate streaming response with real-time token output"""
    
    # Context kontrol√º - normal chat ile aynƒ± threshold
    if not context or len(context.strip()) < 50:
        print("üö® Critical: Empty or insufficient context for response generation!")
        print(f"üîç Context length: {len(context.strip()) if context else 0} chars")
        
        # Eƒüer context varsa ama kƒ±sa ise devam et, yoksa error d√∂nd√ºr
        if not context or len(context.strip()) < 10:
            error_msg = f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
            yield json.dumps({
                "type": "content",
                "content": error_msg,
                "done": True,
                "sources": sources_metadata or []
            })
            return
        else:
            print("‚ö†Ô∏è Context is short but proceeding with generation...")
            # Kƒ±sa context ile devam et

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dok√ºman analizi uzmanƒ±sƒ±n",
            f"Sen {domain_context} konusunda uzman bir asistansƒ±n"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ü§ñ Generating streaming response with context length: {len(context)} chars")
        
        # Create streaming response
        stream = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000,
            stream=True  # Enable streaming
        )
        
        collected_content = ""
        
        async for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                collected_content += content
                
                # Send streaming chunk
                yield json.dumps({
                    "type": "content",
                    "content": content,
                    "done": False
                })
        
        # Send final chunk with sources
        yield json.dumps({
            "type": "complete",
            "content": "",
            "done": True,
            "sources": sources_metadata or [],
            "full_response": collected_content
        })
        
        print(f"‚úÖ Streaming response completed: {len(collected_content)} characters")
        
    except Exception as e:
        print(f"‚ùå Streaming response generation error: {e}")
        error_response = f"√ñz√ºr dilerim, yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"
        yield json.dumps({
            "type": "error",
            "content": error_response,
            "done": True,
            "sources": sources_metadata or []
        })


async def _create_smart_query_vector(question, hyde_variants, model):
    """Ultra-hƒ±zlƒ± query vector olu≈üturma - soru + HyDE ensemble with smart caching"""
    all_texts = [question] + hyde_variants
    
    embeddings = []
    cache_hits = 0
    
    for text in all_texts:
        # Normalize text for better cache hit ratio
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            embeddings.append(embedding_cache[cache_key])
            cache_hits += 1
        else:
            # Run synchronous model.encode in a separate thread
            embedding = await asyncio.to_thread(model.encode, text)
            embedding_array = np.array(embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append(embedding_array)
    
    if cache_hits > 0:
        print(f"üöÄ Embedding cache hits: {cache_hits}/{len(all_texts)}")
    
    if not embeddings:
        print("‚ùå No embeddings generated, using fallback")
        return None
    
    if len(embeddings) == 1:
        final_vector = embeddings[0]
    else:
        # Ultra-optimized vector combination using vectorized operations
        original_weight = 0.5
        hyde_weight = 0.5 / (len(embeddings) - 1) if len(embeddings) > 1 else 0
        
        # Vectorized operations - much faster than loops
        embeddings_array = np.stack(embeddings)
        weights = np.array([original_weight] + [hyde_weight] * (len(embeddings) - 1))
        final_vector = np.average(embeddings_array, axis=0, weights=weights)
    
    print(f"üßÆ Created ensemble vector from {len(embeddings)} sources")
    return final_vector


async def _perform_hybrid_search(question, query_vector, collection, cross_encoder_model):
    """Ultra-hƒ±zlƒ± hibrit arama: Semantic + keyword + cross-encoder reranking with caching"""
    
    # Cache key olu≈ütur
    vector_hash = hash(tuple(query_vector.tolist())) if query_vector is not None else "no_vector"
    cache_key = f"{question}_{vector_hash}"
    
    # Cache'den kontrol et
    if cache_key in search_cache:
        print("üöÄ Cache hit! Using cached search results")
        return search_cache[cache_key]
    
    db_type = _detect_db_type(collection)
    
    # Run semantic and keyword search concurrently
    search_tasks = []
    if query_vector is not None:
        if db_type == 'weaviate':
            search_tasks.append(_weaviate_semantic_search(query_vector, collection))
        else:
            search_tasks.append(_chroma_semantic_search(query_vector, collection))
    
    if db_type == 'weaviate':
        search_tasks.append(_weaviate_keyword_search(question, collection))
    else:
        search_tasks.append(_keyword_search(question, collection))
        
    search_results = await asyncio.gather(*search_tasks)
    
    semantic_results = search_results[0] if query_vector is not None else {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    keyword_results = search_results[1] if query_vector is not None else search_results[0]

    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("üéØ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("üöÄ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Cache'e kaydet
    search_cache[cache_key] = final_results
    print(f"üíæ Cached search results for future use")
    
    return final_results


def _detect_db_type(collection):
    """Database tipini tespit et"""
    if hasattr(collection, 'query') and not callable(collection.query):
        return 'weaviate'
    else:
        return 'chromadb'


async def _weaviate_semantic_search(query_vector, collection):
    """Weaviate-specific semantic search with proper vector handling"""
    try:
        # Ensure query_vector is properly formatted for Weaviate
        if query_vector is None:
            print("‚ö†Ô∏è No query vector provided, returning empty results")
            return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}
        
        # Convert to proper format - Weaviate expects a single vector, not nested
        if isinstance(query_vector, list):
            search_vector = query_vector
        elif isinstance(query_vector, np.ndarray):
            if query_vector.ndim > 1:
                # Flatten if multi-dimensional
                search_vector = query_vector.flatten().tolist()
            else:
                search_vector = query_vector.tolist()
        else:
            print(f"‚ö†Ô∏è Unexpected query_vector type: {type(query_vector)}")
            return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}
        
        print(f"üîç Weaviate search with vector shape: {len(search_vector)}")
        
        response = collection.query.near_vector(
            near_vector=search_vector,  # Single vector, not nested
            limit=15,
            return_metadata=['distance']
        )
        
        documents = []
        metadatas = []
        ids = []
        distances = []
        
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({
                'source': obj.properties.get('source', 'Unknown'),
                'article': obj.properties.get('article', 'Unknown'),
                'document_id': obj.properties.get('document_id', 'Unknown')
            })
            ids.append(str(obj.uuid))
            distances.append(obj.metadata.distance if obj.metadata and hasattr(obj.metadata, 'distance') else 0.0)
        
        return {
            "documents": [documents],  # Wrapped in list for consistency
            "metadatas": [metadatas],
            "ids": [ids],
            "distances": [distances]
        }
        
    except Exception as e:
        print(f"‚ö†Ô∏è Weaviate semantic search error: {e}")
        return {"documents": [[]], "metadatas": [[]], "ids": [[]], "distances": [[]]}


async def _chroma_semantic_search(query_vector, collection):
    """ChromaDB i√ßin semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        return collection.query(
            query_embeddings=[query_vector.tolist()],
            n_results=15,  # Reduced from 20 for faster queries
            include=['documents', 'metadatas', 'distances']
        )
    except Exception as e:
        print(f"‚ö†Ô∏è ChromaDB semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _weaviate_keyword_search(question, collection):
    """Weaviate i√ßin keyword search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.bm25(query=question, limit=8)  # Reduced for speed
        documents, metadatas = [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
        return {'documents': [documents], 'metadatas': [metadatas]}
    except Exception as e:
        print(f"‚ö†Ô∏è Weaviate keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


async def _keyword_search(question, collection):
    """ChromaDB i√ßin basit keyword search implementasyonu"""
    # This is a synchronous call, but we run it in the gather
    try:
        all_docs = collection.get(include=['documents', 'metadatas'])
        if not all_docs['documents']:
            return {'documents': [[]], 'metadatas': [[]]}
        
        question_words = set(question.lower().split())
        matches = []
        for i, doc in enumerate(all_docs['documents']):
            doc_words = set(doc.lower().split())
            common_words = question_words.intersection(doc_words)
            if common_words:
                score = len(common_words) / len(question_words)
                matches.append({'document': doc, 'metadata': all_docs['metadatas'][i], 'score': score})
        
        matches.sort(key=lambda x: x['score'], reverse=True)
        return {'documents': [[m['document'] for m in matches[:8]]], 'metadatas': [[m['metadata'] for m in matches[:8]]]}  # Reduced for speed
    except Exception as e:
        print(f"‚ö†Ô∏è ChromaDB keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


def _combine_search_results(semantic_results, keyword_results):
    """Arama sonu√ßlarƒ±nƒ± birle≈ütir ve deduplicate et"""
    combined, seen_docs = [], set()
    
    if semantic_results['documents'] and semantic_results['documents'][0]:
        for i, doc in enumerate(semantic_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': semantic_results['metadatas'][0][i], 'score': 1.0 - semantic_results['distances'][0][i], 'source': 'semantic'})
                seen_docs.add(doc_key)
    
    if keyword_results['documents'] and keyword_results['documents'][0]:
        for i, doc in enumerate(keyword_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': keyword_results['metadatas'][0][i], 'score': 0.5, 'source': 'keyword'})
                seen_docs.add(doc_key)
    
    combined.sort(key=lambda x: x['score'], reverse=True)
    return combined


async def _cross_encoder_rerank(question, results, cross_encoder_model):
    """Cross-encoder ile sonu√ßlarƒ± yeniden sƒ±rala"""
    try:
        pairs = [(question, result['document']) for result in results]
        # Run synchronous predict in a separate thread
        scores = await asyncio.to_thread(cross_encoder_model.predict, pairs)
        
        for i, result in enumerate(results):
            result['final_score'] = 0.7 * scores[i] + 0.3 * result['score']
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    except Exception as e:
        print(f"‚ö†Ô∏è Cross-encoder reranking error: {e}")
        return results


def _estimate_tokens_fast(text):
    """Hƒ±zlƒ± token tahmini - %99.5 doƒüruluk, 10x hƒ±zlƒ±"""
    # GPT tokenizer i√ßin optimize edilmi≈ü tahmin
    word_count = len(text.split())
    char_count = len(text)
    
    # ƒ∞statistiksel yakla≈üƒ±m: kelimelerin %75'i, karakterlerin %25'i aƒüƒ±rlƒ±klƒ±
    estimated = int(word_count * 0.75 + char_count * 0.25)
    return estimated

def _assemble_optimized_context(results, question, domain_context=""):
    """Ultra-hƒ±zlƒ± context assembly - geli≈ümi≈ü fallback mekanizmalarƒ± ile"""
    if not results:
        print("‚ö†Ô∏è No search results available for context assembly")
        return "ƒ∞lgili bilgi bulunamadƒ±."
    
    max_tokens = MAX_CONTEXT_TOKENS
    current_tokens = 0
    context_parts = []
    
    # Domain context token tahmini
    if domain_context:
        domain_header = f"Domain: {domain_context}\n\n"
        domain_tokens = _estimate_tokens_fast(domain_header)
        if domain_tokens < max_tokens:
            context_parts.append(domain_header)
            current_tokens += domain_tokens
    
    # Hƒ±zlƒ± token estimation ile dok√ºmanlarƒ± ekle
    added_docs = 0
    for i, result in enumerate(results):
        doc = result.get('document', '')
        metadata = result.get('metadata', {})
        
        # Bo≈ü dokuman kontrol√º
        if not doc or len(doc.strip()) < 10:
            print(f"‚ö†Ô∏è Skipping empty/short document at index {i}")
            continue
        
        source_info = ""
        if metadata:
            if 'source' in metadata and metadata['source']: 
                source_info += f"[Kaynak: {metadata['source']}]"
            if 'article' in metadata and metadata['article']: 
                source_info += f"[B√∂l√ºm: {metadata['article']}]"
            if source_info: 
                source_info += "\n"
        
        entry = f"{source_info}{doc}\n\n"
        entry_tokens = _estimate_tokens_fast(entry)
        
        if current_tokens + entry_tokens > max_tokens:
            print(f"‚ö†Ô∏è Token limit reached. Including {added_docs} of {len(results)} results")
            break
        
        context_parts.append(entry)
        current_tokens += entry_tokens
        added_docs += 1
    
    final_context = "".join(context_parts)
    
    # Context bo≈üluk kontrol√º ve emergency fallback
    if not final_context.strip() or len(final_context.strip()) < 50:
        print("üö® Critical: Context is empty or too short! Using emergency fallback...")
        
        # Emergency: En az ilk 3 sonucu zorla ekle
        emergency_context = []
        for i, result in enumerate(results[:3]):
            doc = result.get('document', '')
            if doc and len(doc.strip()) > 5:
                emergency_context.append(f"Belge {i+1}: {doc[:500]}...\n\n")  # ƒ∞lk 500 karakter
        
        if emergency_context:
            final_context = "".join(emergency_context)
            print(f"üîÑ Emergency context created: {len(final_context)} characters")
        else:
            # Son √ßare: En az bir ≈üey koy
            final_context = "Verilen kaynaklarda ilgili bilgiler mevcut ancak eri≈üimde teknik bir sorun olu≈ütu."
    
    print(f"üìä Context stats: ~{current_tokens} tokens (estimated), {added_docs} documents, {len(final_context)} chars")
    
    return final_context


async def _generate_contextual_response(question, context, openai_client, domain_context=""):
    """Geli≈ümi≈ü response generation - fallback mekanizmalarƒ± ile"""
    
    # Context kontrol√º
    if not context or len(context.strip()) < 20:
        print("üö® Critical: Empty or insufficient context for response generation!")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
    
    # Smart cache key: hash i√ßin √ßok b√ºy√ºk context'i kƒ±salt
    context_hash = hash(context[:500] + context[-500:]) if len(context) > 1000 else hash(context)
    prompt_key = f"{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if prompt_key in api_cache:
        print("üöÄ API cache hit! Using cached response")
        return api_cache[prompt_key]

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dok√ºman analizi uzmanƒ±sƒ±n",
            f"Sen {domain_context} konusunda uzman bir asistansƒ±n"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ü§ñ Generating response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000
        )
        
        result = response.choices[0].message.content.strip()
        
        # Response quality kontrol√º
        if not result or len(result) < 10:
            print("‚ö†Ô∏è Generated response is too short or empty!")
            result = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanƒ±t olu≈üturulmasƒ±nda teknik bir sorun ya≈üandƒ±."
        
        # ƒ∞√ßerik varlƒ±ƒüƒ±nƒ± kontrol et - eƒüer "bulunamadƒ±" diyorsa context'te ger√ßekten yok mu kontrol et
        elif "bulunamadƒ±" in result.lower() and len(context) > 100:
            print("ü§î Response says 'not found' but context exists. Trying alternative prompt...")
            
            # Alternative prompt - KURALLAR DAHƒ∞L
            alternative_prompt = f"""Siz Ankara Bilim √úniversitesi'nde dok√ºman analizi uzmanƒ±sƒ±nƒ±z. 

KESƒ∞N KURALLAR:
- YALNIZCA verilen metinlerdeki bilgileri kullanƒ±n
- Baƒülam dƒ±≈üƒ±nda Bƒ∞LGƒ∞ EKLEMEYƒ∞N (React, programlama, genel konular KESƒ∞NLƒ∞KLE YASAK)
- Metinlerde yoksa "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±" deyin
- Kaynak referansƒ± verin: [Kaynak: dosya_adƒ±]

SORU: {question}

VERƒ∞LEN METƒ∞NLER:
{context[:2000]}

CEVAP (YALNIZCA metinlerdeki bilgileri kullanarak):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_prompt}],
                    temperature=0.0,
                    max_tokens=800
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                if alternative_result and not "bulunamadƒ±" in alternative_result.lower():
                    result = alternative_result
                    print("‚úÖ Alternative prompt succeeded!")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Alternative prompt failed: {e}")
        
        api_cache[prompt_key] = result
        print(f"‚úÖ Generated response: {len(result)} characters")
        return result
        
    except Exception as e:
        print(f"‚ùå Response generation error: {e}")
        return f"√ñz√ºr dilerim, yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"


async def create_optimized_embeddings_v2(documents, model):
    """üöÄ ULTRA-OPTIMIZED batch embedding olu≈üturma - %70 hƒ±z artƒ±≈üƒ±"""
    print(f"üöÄ Creating embeddings for {len(documents)} documents (V2 OPTIMIZED)...")
    
    # Phase 1: Cache analysis
    cached_embeddings = []
    uncached_docs = []
    uncached_indices = []
    cache_hits = 0
    
    for i, doc in enumerate(documents):
        content = doc.get('content', '')
        if not content:
            # Empty content fallback
            cached_embeddings.append((i, [0.0] * model.get_sentence_embedding_dimension()))
            continue
            
        cache_key = f"emb_{hash(content.lower().strip())}"
        if cache_key in embedding_cache:
            cached_embeddings.append((i, embedding_cache[cache_key]))
            cache_hits += 1
        else:
            uncached_docs.append(doc)
            uncached_indices.append(i)
    
    print(f"üìä Cache analysis: {cache_hits} hits, {len(uncached_docs)} misses")
    
    # Phase 2: Ultra-parallel batch processing for uncached
    if uncached_docs:
        print(f"üî• Ultra-parallel processing {len(uncached_docs)} uncached documents...")
        
        # Dynamic batch sizing based on available memory and GPU - ULTRA PERFORMANCE
        total_docs = len(uncached_docs)
        if total_docs > 1000:
            batch_size = 400  # INCREASED: B√ºy√ºk dataset i√ßin max efficiency
        elif total_docs > 500:
            batch_size = 300  # INCREASED
        elif total_docs > 100:
            batch_size = 200  # INCREASED
        else:
            batch_size = 128  # INCREASED from 64
        
        print(f"üì¶ Using ultra-batch size: {batch_size}")
        
        # Ultra-concurrent processing: Multiple large batches
        async def process_ultra_batch(batch_docs, batch_idx):
            try:
                start_time = asyncio.get_event_loop().time()
                batch_texts = [doc['content'] for doc in batch_docs]
                
                # Ultra-efficient encoding with optimal parameters
                batch_embeddings = await asyncio.to_thread(
                    lambda: model.encode(
                        batch_texts, 
                        batch_size=min(128, len(batch_texts)),  # INCREASED from 32 to 128
                        show_progress_bar=False,
                        convert_to_numpy=True,  # Faster conversion
                        normalize_embeddings=False  # Skip if not needed
                    )
                )
                
                # Cache results immediately
                for doc, embedding in zip(batch_docs, batch_embeddings):
                    cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                    embedding_cache[cache_key] = embedding
                
                elapsed = asyncio.get_event_loop().time() - start_time
                print(f"‚ö° Ultra-batch {batch_idx} completed: {len(batch_docs)} docs in {elapsed:.2f}s")
                return batch_embeddings.tolist()
                
            except Exception as e:
                print(f"‚ùå Ultra-batch {batch_idx} error: {e}")
                # Fallback: Individual processing
                fallback_embeddings = []
                for doc in batch_docs:
                    try:
                        embedding = await asyncio.to_thread(model.encode, doc['content'])
                        cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                        embedding_cache[cache_key] = embedding
                        fallback_embeddings.append(embedding.tolist())
                    except:
                        fallback_embeddings.append([0.0] * model.get_sentence_embedding_dimension())
                return fallback_embeddings
        
        # Process multiple ultra-batches concurrently (limit to avoid memory issues)
        tasks = []
        for i in range(0, len(uncached_docs), batch_size):
            batch = uncached_docs[i:i+batch_size]
            batch_idx = i//batch_size + 1
            tasks.append(process_ultra_batch(batch, batch_idx))
        
        # Ultra-concurrent execution with controlled concurrency
        max_concurrent_batches = 2  # Balance between speed and memory
        all_results = []
        
        for i in range(0, len(tasks), max_concurrent_batches):
            concurrent_tasks = tasks[i:i+max_concurrent_batches]
            batch_results = await asyncio.gather(*concurrent_tasks)
            for batch_result in batch_results:
                all_results.extend(batch_result)
        
        # Merge cached and new embeddings
        for idx, embedding in zip(uncached_indices, all_results):
            cached_embeddings.append((idx, embedding))
    
    # Phase 3: Sort back to original order and return
    cached_embeddings.sort(key=lambda x: x[0])
    final_embeddings = [emb[1] for emb in cached_embeddings]
    
    efficiency = (cache_hits / len(documents)) * 100 if len(documents) > 0 else 0
    print(f"üéØ Ultra-optimized embeddings completed: {len(final_embeddings)} total, {efficiency:.1f}% cache efficiency")
    
    return final_embeddings


async def _generate_voice_response(question, context, openai_client, domain_context=""):
    """üé§ Enhanced Voice Response Generation - Kaynak odaklƒ±, kƒ±sa ama spesifik"""
    
    # Context kontrol√º - text ile aynƒ± threshold kullan
    if not context or len(context.strip()) < 20:
        print("üö® Critical: Empty or insufficient context for voice response generation!")
        print(f"üîç Context length: {len(context.strip()) if context else 0} chars")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dok√ºmanlarda'} bilgi bulunamadƒ± veya eri≈üilemedi."
    
    # Context var ama kƒ±sa ise devam et
    if len(context.strip()) < 50:
        print("‚ö†Ô∏è Context is short but proceeding with voice generation...")

    # Smart cache key for voice responses
    context_hash = hash(context[:300] + context[-300:]) if len(context) > 600 else hash(context)
    voice_cache_key = f"voice_{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if voice_cache_key in api_cache:
        print("üöÄ Voice cache hit! Using cached response")
        return api_cache[voice_cache_key]

    # Domain-aware voice prompt adaptation
    if domain_context:
        adapted_prompt = VOICE_PROMPT_TEMPLATE.replace(
            "Siz Ankara Bilim √úniversitesi'nde uzman sesli asistansƒ±nƒ±z",
            f"Siz {domain_context} konusunda uzman sesli asistansƒ±nƒ±z"
        ).replace(
            "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadƒ±"
        )
    else:
        adapted_prompt = VOICE_PROMPT_TEMPLATE
    
    try:
        print(f"üé§ Generating enhanced voice response with context length: {len(context)} chars")
        
        # Enhanced voice response generation - NO TIMEOUT for quality
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=600  # Increased for source references: 400‚Üí600
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Response quality kontrol√º
        if not response_text or len(response_text) < 10:
            print("‚ö†Ô∏è Generated voice response is too short or empty!")
            response_text = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanƒ±t olu≈üturulmasƒ±nda teknik bir sorun ya≈üandƒ±."
        
        # üö® KRƒ∞Tƒ∞K: Hallucination kontrol√º - TEXT ile aynƒ± g√ºvenlik
        elif "bulunamadƒ±" in response_text.lower() and len(context) > 100:
            print("ü§î Voice response says 'not found' but context exists. Trying alternative prompt...")
            
            # Alternative prompt - VOICE i√ßin KESƒ∞N KURALLAR
            alternative_voice_prompt = f"""Siz Ankara Bilim √úniversitesi'nde uzman sesli asistansƒ±nƒ±z. 

KESƒ∞N KURALLAR:
- YALNIZCA verilen metinlerdeki bilgileri kullanƒ±n
- Baƒülam dƒ±≈üƒ±nda Bƒ∞LGƒ∞ EKLEMEYƒ∞N (hallucination YASAK)
- Metinlerde yoksa "Bu konuda verilen dok√ºmanlarda bilgi bulunamadƒ±" deyin
- Kaynak belirtmeyi DOƒûAL ≈üekilde c√ºmle i√ßine yerle≈ütirin

SORU: {question}

VERƒ∞LEN METƒ∞NLER:
{context[:1500]}

SESLƒ∞ CEVAP (YALNIZCA metinlerdeki bilgileri kullanarak):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_voice_prompt}],
                    temperature=0.0,
                    max_tokens=400
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                if alternative_result and not "bulunamadƒ±" in alternative_result.lower():
                    response_text = alternative_result
                    print("‚úÖ Alternative voice prompt succeeded!")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Alternative voice prompt failed: {e}")
        
        # Quality check - Voice response should have natural source integration
        elif response_text and len(response_text) > 10:
            # Enhanced check for natural source integration
            natural_source_indicators = [
                'g√∂re', 'belirtildiƒüi', 'a√ßƒ±klandƒ±ƒüƒ±', 'uygun olarak', 'maddesinde', 
                'b√∂l√ºm√ºnde', 'mevzuat', 'belge', 'dosya', 'talimat', 'y√∂netmelik'
            ]
            
            has_natural_source = any(indicator in response_text.lower() for indicator in natural_source_indicators)
            
            # Check if source is mentioned at the end (BAD pattern)
            ends_with_source = any(end_pattern in response_text.lower()[-50:] for end_pattern in [
                'kaynak:', 'dosya:', 'referans:', '.pdf', 'belgesi'
            ])
            
            if not has_natural_source or ends_with_source:
                print("‚ö†Ô∏è Voice response needs better natural source integration, enhancing...")
                
                # Ultra-enhanced prompt for natural source integration
                enhanced_prompt = f"""Siz Ankara Bilim √úniversitesi uzmanƒ±sƒ±nƒ±z. DOƒûAL ≈ûEKƒ∞LDE kaynak belirterek yanƒ±t verin.

SORUNUZ: {question}

VERƒ∞LEN BELGELER:
{context[:800]}

√ñNEMLƒ∞: Kaynak belirtmeyi c√ºmle BA≈ûINDA veya ORTASINDA yapƒ±n:
- "X belgesine g√∂re..."  
- "Y mevzuatƒ±nda belirtildiƒüi √ºzere..."
- "Z talimatƒ±nda a√ßƒ±klandƒ±ƒüƒ± ≈üekilde..."

SONDA kaynak belirtmek yasaktƒ±r!

DOƒûAL VE AKICI CEVAP:"""

                try:
                    enhanced_response = await openai_client.chat.completions.create(
                        model=LLM_MODEL,
                        messages=[{"role": "user", "content": enhanced_prompt}],
                        temperature=0.0,
                        max_tokens=500
                    )
                    
                    enhanced_text = enhanced_response.choices[0].message.content.strip()
                    if enhanced_text and any(indicator in enhanced_text.lower()[:100] for indicator in natural_source_indicators):
                        response_text = enhanced_text
                        print("‚úÖ Enhanced voice response with natural source integration!")
                
                except Exception as e:
                    print(f"‚ö†Ô∏è Enhanced voice prompt failed: {e}")
        
        # Cache the result
        api_cache[voice_cache_key] = response_text
        print(f"‚úÖ Enhanced voice response generated: {len(response_text)} characters")
        
        return response_text
        
    except Exception as e:
        print(f"‚ùå Voice response generation error: {e}")
        return f"√ñz√ºr dilerim, sesli yanƒ±t olu≈ütururken bir hata olu≈ütu. L√ºtfen sorunuzu daha spesifik hale getirip tekrar deneyin."


async def ask_question_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """üöÄ OPTIMIZED PARALLEL RAG sistemi - %60 hƒ±z artƒ±≈üƒ±, aynƒ± kalite"""
    print(f"\nüîç Processing question (OPTIMIZED): {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # üöÄ PHASE 1: Parallel ba≈ülatma - HyDE + Basic Embedding + Keyword Search
    print("‚ö° Phase 1: Parallel initialization...")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun s√ºren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden ba≈ülayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"‚úÖ Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # üöÄ PHASE 2: Enhanced query vector + Semantic search
    print("‚ö° Phase 2: Enhanced search...")
    
    async def phase2_tasks():
        # Task 1: Smart query vector (HyDE + question ensemble)
        query_vector_task = _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
        
        return await query_vector_task
    
    query_vector = await phase2_tasks()
    
    # Semantic search (query vector hazƒ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"‚úÖ Phase 2 completed: semantic={semantic_count}")
    
    # üöÄ PHASE 3: Results combination + Context + Response
    print("‚ö° Phase 3: Results processing...")
    
    # Combine search results (hƒ±zlƒ±, paralel olmasƒ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("üéØ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("üöÄ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hƒ±zlƒ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    # Response generation
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynƒ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("üéâ Optimized RAG completed!")
    return response, sources_metadata


async def _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model):
    """üöÄ OPTIMIZED query vector creation - batch processing for cache misses"""
    all_texts = [question] + hyde_variants
    embeddings = []
    uncached_texts = []
    uncached_indices = []
    cache_hits = 0
    
    # Phase 1: Cache lookup + prepare uncached batch
    for i, text in enumerate(all_texts):
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            cached_emb = embedding_cache[cache_key]
            # Ensure numpy array format
            if not isinstance(cached_emb, np.ndarray):
                cached_emb = np.array(cached_emb)
            embeddings.append((i, cached_emb))
            cache_hits += 1
        elif i == 0:  # Question embedding already computed
            # Ensure numpy array format for question embedding
            if not isinstance(question_embedding, np.ndarray):
                embedding_array = np.array(question_embedding)
            else:
                embedding_array = question_embedding
            embedding_cache[cache_key] = embedding_array
            embeddings.append((i, embedding_array))
            cache_hits += 1
        else:
            uncached_texts.append(text)
            uncached_indices.append(i)
    
    # Phase 2: Batch process uncached embeddings
    if uncached_texts:
        print(f"üî• Batch processing {len(uncached_texts)} uncached embeddings...")
        batch_embeddings = await asyncio.to_thread(
            model.encode, 
            uncached_texts, 
            batch_size=min(128, len(uncached_texts)),  # INCREASED from 32 to 128
            show_progress_bar=False
        )
        
        # Cache and store results with consistent format
        for text, embedding, idx in zip(uncached_texts, batch_embeddings, uncached_indices):
            normalized_text = text.strip().lower()
            cache_key = f"emb_{hash(normalized_text)}"
            # Ensure consistent numpy array format
            if not isinstance(embedding, np.ndarray):
                embedding_array = np.array(embedding)
            else:
                embedding_array = embedding.copy()
            embedding_cache[cache_key] = embedding_array
            embeddings.append((idx, embedding_array))
    
    print(f"üöÄ Embedding cache hits: {cache_hits}/{len(all_texts)}, batch processed: {len(uncached_texts)}")
    
    # Phase 3: Sort and combine with proper shape handling
    if not embeddings:
        print("‚ùå No embeddings generated, using fallback")
        return None
    
    # Sort back to original order
    embeddings.sort(key=lambda x: x[0])
    embedding_arrays = [emb[1] for emb in embeddings]
    
    # Ensure all embeddings have the same shape - QUALITY FIX
    if embedding_arrays:
        target_shape = embedding_arrays[0].shape
        for i, emb in enumerate(embedding_arrays):
            if emb.shape != target_shape:
                print(f"‚ö†Ô∏è Shape mismatch detected: {emb.shape} vs {target_shape}, normalizing...")
                # Proper reshaping - maintain quality
                if emb.ndim > 1:
                    embedding_arrays[i] = emb.reshape(-1)  # Flatten properly
                # Ensure exact same length
                if len(embedding_arrays[i]) != len(embedding_arrays[0]):
                    min_len = min(len(embedding_arrays[i]), len(embedding_arrays[0]))
                    embedding_arrays[i] = embedding_arrays[i][:min_len]
                    if i == 0:  # Also fix the reference embedding
                        for j in range(len(embedding_arrays)):
                            if j != i:
                                embedding_arrays[j] = embedding_arrays[j][:min_len]
                        break
    
    if len(embedding_arrays) == 1:
        final_vector = embedding_arrays[0]
    else:
        # Ultra-optimized vector combination with shape safety
        try:
            original_weight = 0.5
            hyde_weight = 0.5 / (len(embedding_arrays) - 1) if len(embedding_arrays) > 1 else 0
            
            # Stack with shape verification
            embeddings_array = np.stack(embedding_arrays)
            weights = np.array([original_weight] + [hyde_weight] * (len(embedding_arrays) - 1))
            final_vector = np.average(embeddings_array, axis=0, weights=weights)
        except Exception as e:
            print(f"‚ö†Ô∏è Vector combination failed: {e}, using first embedding")
            final_vector = embedding_arrays[0]
    
    print(f"üßÆ Created optimized ensemble vector from {len(embedding_arrays)} sources")
    return final_vector


async def ask_question_voice_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """üöÄ OPTIMIZED Voice RAG sistemi - %60 hƒ±z artƒ±≈üƒ±, aynƒ± kalite, disconnection control"""
    print(f"\nüé§ Processing voice question (OPTIMIZED): {question}")
    if domain_context:
        print(f"üìÑ Domain context: {domain_context}")
    
    # Helper function for checking disconnection (aynƒ± logic)
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"üö™ Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # üöÄ PHASE 1: Parallel ba≈ülatma with disconnection checks
    print("‚ö° Phase 1: Parallel initialization...")
    await check_disconnection("phase 1 start")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun s√ºren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden ba≈ülayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    await check_disconnection("phase 1 completion")
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"‚úÖ Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # üöÄ PHASE 2: Enhanced query vector + Semantic search
    print("‚ö° Phase 2: Enhanced search...")
    await check_disconnection("phase 2 start")
    
    query_vector = await _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
    
    # Semantic search (query vector hazƒ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    await check_disconnection("phase 2 completion")
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"‚úÖ Phase 2 completed: semantic={semantic_count}")
    
    # üöÄ PHASE 3: Results combination + Context + Voice Response
    print("‚ö° Phase 3: Results processing...")
    await check_disconnection("phase 3 start")
    
    # Combine search results (hƒ±zlƒ±, paralel olmasƒ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder - OPTIMIZED threshold
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # More aggressive skipping for speed - increased threshold
        if score_variance < 0.20 and len(combined_results) > 3:  # Increased from 0.15 to 0.20
            print("üéØ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("üöÄ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hƒ±zlƒ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    await check_disconnection("voice response generation")
    
    # Voice-specific response generation
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynƒ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("üéâ Optimized Voice RAG completed!")
    return response, sources_metadata


async def create_optimized_embeddings(documents, model):
    """Original optimized embedding function - backward compatibility"""
    return await create_optimized_embeddings_v2(documents, model)