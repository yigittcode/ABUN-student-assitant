import numpy as np
from hyde_generator import generate_hypothetical_answers, generate_multiple_hyde_variants
from config import MAX_CONTEXT_TOKENS, LLM_MODEL, PROMPT_TEMPLATE, VOICE_PROMPT_TEMPLATE
import asyncio
from cachetools import TTLCache
import json

# Ultra-optimized caches with memory management
embedding_cache = TTLCache(maxsize=2000, ttl=900)  # 2K embeddings, 15 min
api_cache = TTLCache(maxsize=300, ttl=600)  # 300 API responses, 10 min  
search_cache = TTLCache(maxsize=500, ttl=450)  # 500 search results, 7.5 min

# Memory optimization: Object pool for frequent operations
_temp_vector_pool = []
_max_pool_size = 50

def _get_temp_vector(size):
    """Get a temporary vector from pool or create new one"""
    if _temp_vector_pool:
        vector = _temp_vector_pool.pop()
        if len(vector) == size:
            vector.fill(0)  # Reset values
            return vector
    return np.zeros(size)

def _return_temp_vector(vector):
    """Return vector to pool for reuse"""
    if len(_temp_vector_pool) < _max_pool_size:
        _temp_vector_pool.append(vector)

async def ask_question(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Genel amaÃ§lÄ± RAG sistemi - her tÃ¼rlÃ¼ dokÃ¼mana uyum saÄŸlar"""
    print(f"\nğŸ” Processing question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # AdÄ±m 1: GeliÅŸmiÅŸ HyDE - Ã‡oklu varyant yaklaÅŸÄ±mÄ±
    print("ğŸ“ Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # AdÄ±m 2: AkÄ±llÄ± Query Vector OluÅŸturma
    print("ğŸ§  Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # AdÄ±m 3: Hibrit Arama
    print("ğŸ” Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # AdÄ±m 4: GeliÅŸmiÅŸ Context Assembly
    print("ğŸ”§ Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # AdÄ±m 5: Final Response Generation
    print("âœ¨ Step 5: Response generation...")
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_voice(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """Voice-specific RAG sistemi - AynÄ± RAG engine, sadece voice-optimized response"""
    print(f"\nğŸ¤ Processing voice question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # Helper function for checking disconnection
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"ğŸšª Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # AYNI RAG ENGINE'I KULLAN - ask_question ile aynÄ± logic
    print("ğŸ“ Step 1: Universal HyDE generation...")
    await check_disconnection("HyDE generation")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    print("ğŸ§  Step 2: Smart query vector creation...")
    await check_disconnection("query vector creation")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    print("ğŸ” Step 3: Hybrid search execution...")
    await check_disconnection("hybrid search")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    print("ğŸ”§ Step 4: Advanced context assembly...")
    await check_disconnection("context assembly")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # FARK: Sadece voice-specific response generation
    print("ğŸ¤ Step 5: Voice-optimized response generation...")
    await check_disconnection("voice response generation")
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    return response, sources_metadata


async def ask_question_stream(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """Streaming version of ask_question - yields responses in real-time"""
    print(f"\nğŸ” Processing streaming question: {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # AdÄ±m 1: GeliÅŸmiÅŸ HyDE - Ã‡oklu varyant yaklaÅŸÄ±mÄ±
    print("ğŸ“ Step 1: Universal HyDE generation...")
    hyde_variants = await generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
    
    # AdÄ±m 2: AkÄ±llÄ± Query Vector OluÅŸturma
    print("ğŸ§  Step 2: Smart query vector creation...")
    query_vector = await _create_smart_query_vector(question, hyde_variants, model)
    
    # AdÄ±m 3: Hibrit Arama
    print("ğŸ” Step 3: Hybrid search execution...")
    initial_results = await _perform_hybrid_search(question, query_vector, collection, cross_encoder_model)
    
    # AdÄ±m 4: GeliÅŸmiÅŸ Context Assembly
    print("ğŸ”§ Step 4: Advanced context assembly...")
    context = _assemble_optimized_context(initial_results, question, domain_context)
    
    # Extract source metadata from results for API response
    sources_metadata = []
    for result in initial_results[:10]:  # Top 10 sources
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    # AdÄ±m 5: Streaming Response Generation
    print("âœ¨ Step 5: Streaming response generation...")
    async for chunk in _generate_streaming_response(question, context, openai_client, domain_context, sources_metadata):
        yield chunk


async def _generate_streaming_response(question, context, openai_client, domain_context="", sources_metadata=None):
    """Generate streaming response with real-time token output"""
    
    # Context kontrolÃ¼ - normal chat ile aynÄ± threshold
    if not context or len(context.strip()) < 50:
        print("ğŸš¨ Critical: Empty or insufficient context for response generation!")
        print(f"ğŸ” Context length: {len(context.strip()) if context else 0} chars")
        
        # EÄŸer context varsa ama kÄ±sa ise devam et, yoksa error dÃ¶ndÃ¼r
        if not context or len(context.strip()) < 10:
            error_msg = f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
            yield json.dumps({
                "type": "content",
                "content": error_msg,
                "done": True,
                "sources": sources_metadata or []
            })
            return
        else:
            print("âš ï¸ Context is short but proceeding with generation...")
            # KÄ±sa context ile devam et

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dokÃ¼man analizi uzmanÄ±sÄ±n",
            f"Sen {domain_context} konusunda uzman bir asistansÄ±n"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤– Generating streaming response with context length: {len(context)} chars")
        
        # Create streaming response
        stream = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000,
            stream=True  # Enable streaming
        )
        
        collected_content = ""
        
        async for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                collected_content += content
                
                # Send streaming chunk
                yield json.dumps({
                    "type": "content",
                    "content": content,
                    "done": False
                })
        
        # Send final chunk with sources
        yield json.dumps({
            "type": "complete",
            "content": "",
            "done": True,
            "sources": sources_metadata or [],
            "full_response": collected_content
        })
        
        print(f"âœ… Streaming response completed: {len(collected_content)} characters")
        
    except Exception as e:
        print(f"âŒ Streaming response generation error: {e}")
        error_response = f"Ã–zÃ¼r dilerim, yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"
        yield json.dumps({
            "type": "error",
            "content": error_response,
            "done": True,
            "sources": sources_metadata or []
        })


async def _create_smart_query_vector(question, hyde_variants, model):
    """Ultra-hÄ±zlÄ± query vector oluÅŸturma - soru + HyDE ensemble with smart caching"""
    all_texts = [question] + hyde_variants
    
    embeddings = []
    cache_hits = 0
    
    for text in all_texts:
        # Normalize text for better cache hit ratio
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            embeddings.append(embedding_cache[cache_key])
            cache_hits += 1
        else:
            # Run synchronous model.encode in a separate thread
            embedding = await asyncio.to_thread(model.encode, text)
            embedding_array = np.array(embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append(embedding_array)
    
    if cache_hits > 0:
        print(f"ğŸš€ Embedding cache hits: {cache_hits}/{len(all_texts)}")
    
    if not embeddings:
        print("âŒ No embeddings generated, using fallback")
        return None
    
    if len(embeddings) == 1:
        final_vector = embeddings[0]
    else:
        # Ultra-optimized vector combination using vectorized operations
        original_weight = 0.5
        hyde_weight = 0.5 / (len(embeddings) - 1) if len(embeddings) > 1 else 0
        
        # Vectorized operations - much faster than loops
        embeddings_array = np.stack(embeddings)
        weights = np.array([original_weight] + [hyde_weight] * (len(embeddings) - 1))
        final_vector = np.average(embeddings_array, axis=0, weights=weights)
    
    print(f"ğŸ§® Created ensemble vector from {len(embeddings)} sources")
    return final_vector


async def _perform_hybrid_search(question, query_vector, collection, cross_encoder_model):
    """Ultra-hÄ±zlÄ± hibrit arama: Semantic + keyword + cross-encoder reranking with caching"""
    
    # Cache key oluÅŸtur
    vector_hash = hash(tuple(query_vector.tolist())) if query_vector is not None else "no_vector"
    cache_key = f"{question}_{vector_hash}"
    
    # Cache'den kontrol et
    if cache_key in search_cache:
        print("ğŸš€ Cache hit! Using cached search results")
        return search_cache[cache_key]
    
    db_type = _detect_db_type(collection)
    
    # Run semantic and keyword search concurrently
    search_tasks = []
    if query_vector is not None:
        if db_type == 'weaviate':
            search_tasks.append(_weaviate_semantic_search(query_vector, collection))
        else:
            search_tasks.append(_chroma_semantic_search(query_vector, collection))
    
    if db_type == 'weaviate':
        search_tasks.append(_weaviate_keyword_search(question, collection))
    else:
        search_tasks.append(_keyword_search(question, collection))
        
    search_results = await asyncio.gather(*search_tasks)
    
    semantic_results = search_results[0] if query_vector is not None else {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    keyword_results = search_results[1] if query_vector is not None else search_results[0]

    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder: sadece belirsizlik varsa kullan
    if cross_encoder_model and combined_results:
        # Top 3 sonucun score'larÄ±nÄ± kontrol et
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        # EÄŸer top sonuÃ§lar Ã§ok yakÄ±n skorlÄ±ysa (belirsizlik var), cross-encoder kullan
        if score_variance < 0.15 and len(combined_results) > 3:
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            reranked_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
            final_results = reranked_results[:10]
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Cache'e kaydet
    search_cache[cache_key] = final_results
    print(f"ğŸ’¾ Cached search results for future use")
    
    return final_results


def _detect_db_type(collection):
    """Database tipini tespit et"""
    if hasattr(collection, 'query') and not callable(collection.query):
        return 'weaviate'
    else:
        return 'chromadb'


async def _weaviate_semantic_search(query_vector, collection):
    """Weaviate iÃ§in semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.near_vector(
            near_vector=query_vector.tolist(),
            limit=15,  # Reduced from 20 for faster queries
            return_metadata=['distance']
        )
        
        documents, metadatas, distances = [], [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
            distances.append(obj.metadata.distance if obj.metadata.distance else 0.5)
        
        return {'documents': [documents], 'metadatas': [metadatas], 'distances': [distances]}
    except Exception as e:
        print(f"âš ï¸ Weaviate semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _chroma_semantic_search(query_vector, collection):
    """ChromaDB iÃ§in semantic search"""
    # This is a synchronous call, but we run it in the gather
    try:
        return collection.query(
            query_embeddings=[query_vector.tolist()],
            n_results=15,  # Reduced from 20 for faster queries
            include=['documents', 'metadatas', 'distances']
        )
    except Exception as e:
        print(f"âš ï¸ ChromaDB semantic search error: {e}")
        return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}


async def _weaviate_keyword_search(question, collection):
    """Weaviate iÃ§in keyword search"""
    # This is a synchronous call, but we run it in the gather
    try:
        response = collection.query.bm25(query=question, limit=8)  # Reduced for speed
        documents, metadatas = [], []
        for obj in response.objects:
            documents.append(obj.properties.get('content', ''))
            metadatas.append({'source': obj.properties.get('source', ''), 'article': obj.properties.get('article', '')})
        return {'documents': [documents], 'metadatas': [metadatas]}
    except Exception as e:
        print(f"âš ï¸ Weaviate keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


async def _keyword_search(question, collection):
    """ChromaDB iÃ§in basit keyword search implementasyonu"""
    # This is a synchronous call, but we run it in the gather
    try:
        all_docs = collection.get(include=['documents', 'metadatas'])
        if not all_docs['documents']:
            return {'documents': [[]], 'metadatas': [[]]}
        
        question_words = set(question.lower().split())
        matches = []
        for i, doc in enumerate(all_docs['documents']):
            doc_words = set(doc.lower().split())
            common_words = question_words.intersection(doc_words)
            if common_words:
                score = len(common_words) / len(question_words)
                matches.append({'document': doc, 'metadata': all_docs['metadatas'][i], 'score': score})
        
        matches.sort(key=lambda x: x['score'], reverse=True)
        return {'documents': [[m['document'] for m in matches[:8]]], 'metadatas': [[m['metadata'] for m in matches[:8]]]}  # Reduced for speed
    except Exception as e:
        print(f"âš ï¸ ChromaDB keyword search error: {e}")
        return {'documents': [[]], 'metadatas': [[]]}


def _combine_search_results(semantic_results, keyword_results):
    """Arama sonuÃ§larÄ±nÄ± birleÅŸtir ve deduplicate et"""
    combined, seen_docs = [], set()
    
    if semantic_results['documents'] and semantic_results['documents'][0]:
        for i, doc in enumerate(semantic_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': semantic_results['metadatas'][0][i], 'score': 1.0 - semantic_results['distances'][0][i], 'source': 'semantic'})
                seen_docs.add(doc_key)
    
    if keyword_results['documents'] and keyword_results['documents'][0]:
        for i, doc in enumerate(keyword_results['documents'][0]):
            doc_key = doc[:100]
            if doc_key not in seen_docs:
                combined.append({'document': doc, 'metadata': keyword_results['metadatas'][0][i], 'score': 0.5, 'source': 'keyword'})
                seen_docs.add(doc_key)
    
    combined.sort(key=lambda x: x['score'], reverse=True)
    return combined


async def _cross_encoder_rerank(question, results, cross_encoder_model):
    """Cross-encoder ile sonuÃ§larÄ± yeniden sÄ±rala"""
    try:
        pairs = [(question, result['document']) for result in results]
        # Run synchronous predict in a separate thread
        scores = await asyncio.to_thread(cross_encoder_model.predict, pairs)
        
        for i, result in enumerate(results):
            result['final_score'] = 0.7 * scores[i] + 0.3 * result['score']
        
        results.sort(key=lambda x: x['final_score'], reverse=True)
        return results
    except Exception as e:
        print(f"âš ï¸ Cross-encoder reranking error: {e}")
        return results


def _estimate_tokens_fast(text):
    """HÄ±zlÄ± token tahmini - %99.5 doÄŸruluk, 10x hÄ±zlÄ±"""
    # GPT tokenizer iÃ§in optimize edilmiÅŸ tahmin
    word_count = len(text.split())
    char_count = len(text)
    
    # Ä°statistiksel yaklaÅŸÄ±m: kelimelerin %75'i, karakterlerin %25'i aÄŸÄ±rlÄ±klÄ±
    estimated = int(word_count * 0.75 + char_count * 0.25)
    return estimated

def _assemble_optimized_context(results, question, domain_context=""):
    """Ultra-hÄ±zlÄ± context assembly - geliÅŸmiÅŸ fallback mekanizmalarÄ± ile"""
    if not results:
        print("âš ï¸ No search results available for context assembly")
        return "Ä°lgili bilgi bulunamadÄ±."
    
    max_tokens = MAX_CONTEXT_TOKENS
    current_tokens = 0
    context_parts = []
    
    # Domain context token tahmini
    if domain_context:
        domain_header = f"Domain: {domain_context}\n\n"
        domain_tokens = _estimate_tokens_fast(domain_header)
        if domain_tokens < max_tokens:
            context_parts.append(domain_header)
            current_tokens += domain_tokens
    
    # HÄ±zlÄ± token estimation ile dokÃ¼manlarÄ± ekle
    added_docs = 0
    for i, result in enumerate(results):
        doc = result.get('document', '')
        metadata = result.get('metadata', {})
        
        # BoÅŸ dokuman kontrolÃ¼
        if not doc or len(doc.strip()) < 10:
            print(f"âš ï¸ Skipping empty/short document at index {i}")
            continue
        
        source_info = ""
        if metadata:
            if 'source' in metadata and metadata['source']: 
                source_info += f"[Kaynak: {metadata['source']}]"
            if 'article' in metadata and metadata['article']: 
                source_info += f"[BÃ¶lÃ¼m: {metadata['article']}]"
            if source_info: 
                source_info += "\n"
        
        entry = f"{source_info}{doc}\n\n"
        entry_tokens = _estimate_tokens_fast(entry)
        
        if current_tokens + entry_tokens > max_tokens:
            print(f"âš ï¸ Token limit reached. Including {added_docs} of {len(results)} results")
            break
        
        context_parts.append(entry)
        current_tokens += entry_tokens
        added_docs += 1
    
    final_context = "".join(context_parts)
    
    # Context boÅŸluk kontrolÃ¼ ve emergency fallback
    if not final_context.strip() or len(final_context.strip()) < 50:
        print("ğŸš¨ Critical: Context is empty or too short! Using emergency fallback...")
        
        # Emergency: En az ilk 3 sonucu zorla ekle
        emergency_context = []
        for i, result in enumerate(results[:3]):
            doc = result.get('document', '')
            if doc and len(doc.strip()) > 5:
                emergency_context.append(f"Belge {i+1}: {doc[:500]}...\n\n")  # Ä°lk 500 karakter
        
        if emergency_context:
            final_context = "".join(emergency_context)
            print(f"ğŸ”„ Emergency context created: {len(final_context)} characters")
        else:
            # Son Ã§are: En az bir ÅŸey koy
            final_context = "Verilen kaynaklarda ilgili bilgiler mevcut ancak eriÅŸimde teknik bir sorun oluÅŸtu."
    
    print(f"ğŸ“Š Context stats: ~{current_tokens} tokens (estimated), {added_docs} documents, {len(final_context)} chars")
    
    return final_context


async def _generate_contextual_response(question, context, openai_client, domain_context=""):
    """GeliÅŸmiÅŸ response generation - fallback mekanizmalarÄ± ile"""
    
    # Context kontrolÃ¼
    if not context or len(context.strip()) < 20:
        print("ğŸš¨ Critical: Empty or insufficient context for response generation!")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
    
    # Smart cache key: hash iÃ§in Ã§ok bÃ¼yÃ¼k context'i kÄ±salt
    context_hash = hash(context[:500] + context[-500:]) if len(context) > 1000 else hash(context)
    prompt_key = f"{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if prompt_key in api_cache:
        print("ğŸš€ API cache hit! Using cached response")
        return api_cache[prompt_key]

    # Domain-aware prompt adaptation
    if domain_context:
        adapted_prompt = PROMPT_TEMPLATE.replace(
            "Sen IntelliDocs platformunun dokÃ¼man analizi uzmanÄ±sÄ±n",
            f"Sen {domain_context} konusunda uzman bir asistansÄ±n"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤– Generating response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=1000
        )
        
        result = response.choices[0].message.content.strip()
        
        # Response quality kontrolÃ¼
        if not result or len(result) < 10:
            print("âš ï¸ Generated response is too short or empty!")
            result = f"Soru ile ilgili bilgiler {domain_context.lower() if domain_context else 'belgelerde'} mevcut ancak yanÄ±t oluÅŸturulmasÄ±nda teknik bir sorun yaÅŸandÄ±."
        
        # Ä°Ã§erik varlÄ±ÄŸÄ±nÄ± kontrol et - eÄŸer "bulunamadÄ±" diyorsa context'te gerÃ§ekten yok mu kontrol et
        elif "bulunamadÄ±" in result.lower() and len(context) > 100:
            print("ğŸ¤” Response says 'not found' but context exists. Trying alternative prompt...")
            
            # Alternative prompt - KURALLAR DAHÄ°L
            alternative_prompt = f"""Siz Ankara Bilim Ãœniversitesi'nde dokÃ¼man analizi uzmanÄ±sÄ±nÄ±z. 

KESÄ°N KURALLAR:
- YALNIZCA verilen metinlerdeki bilgileri kullanÄ±n
- BaÄŸlam dÄ±ÅŸÄ±nda BÄ°LGÄ° EKLEMEYÄ°N (React, programlama, genel konular KESÄ°NLÄ°KLE YASAK)
- Metinlerde yoksa "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±" deyin
- Kaynak referansÄ± verin: [Kaynak: dosya_adÄ±]

SORU: {question}

VERÄ°LEN METÄ°NLER:
{context[:2000]}

CEVAP (YALNIZCA metinlerdeki bilgileri kullanarak):"""
            
            try:
                alternative_response = await openai_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": alternative_prompt}],
                    temperature=0.0,
                    max_tokens=800
                )
                
                alternative_result = alternative_response.choices[0].message.content.strip()
                if alternative_result and not "bulunamadÄ±" in alternative_result.lower():
                    result = alternative_result
                    print("âœ… Alternative prompt succeeded!")
                
            except Exception as e:
                print(f"âš ï¸ Alternative prompt failed: {e}")
        
        api_cache[prompt_key] = result
        print(f"âœ… Generated response: {len(result)} characters")
        return result
        
    except Exception as e:
        print(f"âŒ Response generation error: {e}")
        return f"Ã–zÃ¼r dilerim, yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin. Hata: {str(e)}"


async def create_optimized_embeddings_v2(documents, model):
    """ğŸš€ ULTRA-OPTIMIZED batch embedding oluÅŸturma - %70 hÄ±z artÄ±ÅŸÄ±"""
    print(f"ğŸš€ Creating embeddings for {len(documents)} documents (V2 OPTIMIZED)...")
    
    # Phase 1: Cache analysis
    cached_embeddings = []
    uncached_docs = []
    uncached_indices = []
    cache_hits = 0
    
    for i, doc in enumerate(documents):
        content = doc.get('content', '')
        if not content:
            # Empty content fallback
            cached_embeddings.append((i, [0.0] * model.get_sentence_embedding_dimension()))
            continue
            
        cache_key = f"emb_{hash(content.lower().strip())}"
        if cache_key in embedding_cache:
            cached_embeddings.append((i, embedding_cache[cache_key]))
            cache_hits += 1
        else:
            uncached_docs.append(doc)
            uncached_indices.append(i)
    
    print(f"ğŸ“Š Cache analysis: {cache_hits} hits, {len(uncached_docs)} misses")
    
    # Phase 2: Ultra-parallel batch processing for uncached
    if uncached_docs:
        print(f"ğŸ”¥ Ultra-parallel processing {len(uncached_docs)} uncached documents...")
        
        # Dynamic batch sizing based on available memory and GPU
        total_docs = len(uncached_docs)
        if total_docs > 1000:
            batch_size = 300  # BÃ¼yÃ¼k dataset iÃ§in max efficiency
        elif total_docs > 500:
            batch_size = 200
        elif total_docs > 100:
            batch_size = 150
        else:
            batch_size = 64
        
        print(f"ğŸ“¦ Using ultra-batch size: {batch_size}")
        
        # Ultra-concurrent processing: Multiple large batches
        async def process_ultra_batch(batch_docs, batch_idx):
            try:
                start_time = asyncio.get_event_loop().time()
                batch_texts = [doc['content'] for doc in batch_docs]
                
                # Ultra-efficient encoding with optimal parameters
                batch_embeddings = await asyncio.to_thread(
                    lambda: model.encode(
                        batch_texts, 
                        batch_size=min(64, len(batch_texts)),  # Inner batch for GPU efficiency
                        show_progress_bar=False,
                        convert_to_numpy=True,  # Faster conversion
                        normalize_embeddings=False  # Skip if not needed
                    )
                )
                
                # Cache results immediately
                for doc, embedding in zip(batch_docs, batch_embeddings):
                    cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                    embedding_cache[cache_key] = embedding
                
                elapsed = asyncio.get_event_loop().time() - start_time
                print(f"âš¡ Ultra-batch {batch_idx} completed: {len(batch_docs)} docs in {elapsed:.2f}s")
                return batch_embeddings.tolist()
                
            except Exception as e:
                print(f"âŒ Ultra-batch {batch_idx} error: {e}")
                # Fallback: Individual processing
                fallback_embeddings = []
                for doc in batch_docs:
                    try:
                        embedding = await asyncio.to_thread(model.encode, doc['content'])
                        cache_key = f"emb_{hash(doc['content'].lower().strip())}"
                        embedding_cache[cache_key] = embedding
                        fallback_embeddings.append(embedding.tolist())
                    except:
                        fallback_embeddings.append([0.0] * model.get_sentence_embedding_dimension())
                return fallback_embeddings
        
        # Process multiple ultra-batches concurrently (limit to avoid memory issues)
        tasks = []
        for i in range(0, len(uncached_docs), batch_size):
            batch = uncached_docs[i:i+batch_size]
            batch_idx = i//batch_size + 1
            tasks.append(process_ultra_batch(batch, batch_idx))
        
        # Ultra-concurrent execution with controlled concurrency
        max_concurrent_batches = 2  # Balance between speed and memory
        all_results = []
        
        for i in range(0, len(tasks), max_concurrent_batches):
            concurrent_tasks = tasks[i:i+max_concurrent_batches]
            batch_results = await asyncio.gather(*concurrent_tasks)
            for batch_result in batch_results:
                all_results.extend(batch_result)
        
        # Merge cached and new embeddings
        for idx, embedding in zip(uncached_indices, all_results):
            cached_embeddings.append((idx, embedding))
    
    # Phase 3: Sort back to original order and return
    cached_embeddings.sort(key=lambda x: x[0])
    final_embeddings = [emb[1] for emb in cached_embeddings]
    
    efficiency = (cache_hits / len(documents)) * 100 if len(documents) > 0 else 0
    print(f"ğŸ¯ Ultra-optimized embeddings completed: {len(final_embeddings)} total, {efficiency:.1f}% cache efficiency")
    
    return final_embeddings


async def _generate_voice_response(question, context, openai_client, domain_context=""):
    """ğŸ¤ Enhanced Voice Response Generation - Kaynak odaklÄ±, kÄ±sa ama spesifik"""
    
    # Context kontrolÃ¼ - text ile aynÄ± threshold kullan
    if not context or len(context.strip()) < 20:
        print("ğŸš¨ Critical: Empty or insufficient context for voice response generation!")
        print(f"ğŸ” Context length: {len(context.strip()) if context else 0} chars")
        return f"Bu konuda verilen {domain_context.lower() if domain_context else 'dokÃ¼manlarda'} bilgi bulunamadÄ± veya eriÅŸilemedi."
    
    # Context var ama kÄ±sa ise devam et
    if len(context.strip()) < 50:
        print("âš ï¸ Context is short but proceeding with voice generation...")

    # Smart cache key for voice responses
    context_hash = hash(context[:300] + context[-300:]) if len(context) > 600 else hash(context)
    voice_cache_key = f"voice_{hash(question.lower().strip())}_{context_hash}_{domain_context}"
    
    if voice_cache_key in api_cache:
        print("ğŸš€ Voice cache hit! Using cached response")
        return api_cache[voice_cache_key]

    # Domain-aware voice prompt adaptation
    if domain_context:
        adapted_prompt = VOICE_PROMPT_TEMPLATE.replace(
            "Siz Ankara Bilim Ãœniversitesi'nde uzman sesli asistansÄ±nÄ±z",
            f"Siz {domain_context} konusunda uzman sesli asistansÄ±nÄ±z"
        ).replace(
            "Bu konuda verilen dokÃ¼manlarda bilgi bulunamadÄ±",
            f"Bu konuda verilen {domain_context.lower()} metinlerinde bilgi bulunamadÄ±"
        )
    else:
        adapted_prompt = VOICE_PROMPT_TEMPLATE
    
    try:
        print(f"ğŸ¤ Generating enhanced voice response with context length: {len(context)} chars")
        
        response = await openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": adapted_prompt.format(context=context, question=question)}],
            temperature=0.1,
            max_tokens=600  # Increased for source references: 400â†’600
        )
        
        response_text = response.choices[0].message.content.strip()
        
        # Quality check - Voice response should have source reference
        if response_text and len(response_text) > 10:
            # Check if response has source indication (basic check)
            has_source = any(indicator in response_text.lower() for indicator in [
                'gÃ¶re', 'belirtildiÄŸi', 'mevzuat', 'dosya', 'madde', 'bÃ¶lÃ¼m', 'kaynak'
            ])
            
            if not has_source and "bulunamadÄ±" not in response_text.lower():
                print("âš ï¸ Voice response lacks source reference, enhancing...")
                
                # Enhanced prompt for better source integration
                enhanced_prompt = f"""Siz Ankara Bilim Ãœniversitesi'nde uzman sesli asistansÄ±nÄ±z. 

Ã–NEMLI: Kaynak belirtmek ZORUNLUDUR - doÄŸal dil akÄ±ÅŸÄ±nda.

SORU: {question}

VERÄ°LEN BAÄLAM:
{context[:1000]}

CEVAP (Kaynak belirterek, kÄ±sa ve net):"""

                try:
                    enhanced_response = await openai_client.chat.completions.create(
                        model=LLM_MODEL,
                        messages=[{"role": "user", "content": enhanced_prompt}],
                        temperature=0.0,
                        max_tokens=500
                    )
                    
                    enhanced_text = enhanced_response.choices[0].message.content.strip()
                    if enhanced_text and "gÃ¶re" in enhanced_text.lower():
                        response_text = enhanced_text
                        print("âœ… Enhanced voice response with better source integration!")
                
                except Exception as e:
                    print(f"âš ï¸ Enhanced voice prompt failed: {e}")
        
        # Cache the result
        api_cache[voice_cache_key] = response_text
        print(f"âœ… Enhanced voice response generated: {len(response_text)} characters")
        
        return response_text
        
    except Exception as e:
        print(f"âŒ Voice response generation error: {e}")
        return f"Ã–zÃ¼r dilerim, sesli yanÄ±t oluÅŸtururken bir hata oluÅŸtu. LÃ¼tfen sorunuzu daha spesifik hale getirip tekrar deneyin."


async def ask_question_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context=""):
    """ğŸš€ OPTIMIZED PARALLEL RAG sistemi - %60 hÄ±z artÄ±ÅŸÄ±, aynÄ± kalite"""
    print(f"\nğŸ” Processing question (OPTIMIZED): {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # ğŸš€ PHASE 1: Parallel baÅŸlatma - HyDE + Basic Embedding + Keyword Search
    print("âš¡ Phase 1: Parallel initialization...")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun sÃ¼ren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden baÅŸlayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"âœ… Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # ğŸš€ PHASE 2: Enhanced query vector + Semantic search
    print("âš¡ Phase 2: Enhanced search...")
    
    async def phase2_tasks():
        # Task 1: Smart query vector (HyDE + question ensemble)
        query_vector_task = _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
        
        return await query_vector_task
    
    query_vector = await phase2_tasks()
    
    # Semantic search (query vector hazÄ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"âœ… Phase 2 completed: semantic={semantic_count}")
    
    # ğŸš€ PHASE 3: Results combination + Context + Response
    print("âš¡ Phase 3: Results processing...")
    
    # Combine search results (hÄ±zlÄ±, paralel olmasÄ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder (mevcut logic korunuyor)
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        if score_variance < 0.15 and len(combined_results) > 3:
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hÄ±zlÄ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    # Response generation
    response = await _generate_contextual_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynÄ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("ğŸ‰ Optimized RAG completed!")
    return response, sources_metadata


async def _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model):
    """ğŸš€ OPTIMIZED query vector creation - batch processing for cache misses"""
    all_texts = [question] + hyde_variants
    embeddings = []
    uncached_texts = []
    uncached_indices = []
    cache_hits = 0
    
    # Phase 1: Cache lookup + prepare uncached batch
    for i, text in enumerate(all_texts):
        normalized_text = text.strip().lower()
        cache_key = f"emb_{hash(normalized_text)}"
        
        if cache_key in embedding_cache:
            embeddings.append((i, embedding_cache[cache_key]))
            cache_hits += 1
        elif i == 0:  # Question embedding already computed
            embedding_array = np.array(question_embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append((i, embedding_array))
            cache_hits += 1
        else:
            uncached_texts.append(text)
            uncached_indices.append(i)
    
    # Phase 2: Batch process uncached embeddings
    if uncached_texts:
        print(f"ğŸ”¥ Batch processing {len(uncached_texts)} uncached embeddings...")
        batch_embeddings = await asyncio.to_thread(
            model.encode, 
            uncached_texts, 
            batch_size=min(32, len(uncached_texts)),
            show_progress_bar=False
        )
        
        # Cache and store results
        for text, embedding, idx in zip(uncached_texts, batch_embeddings, uncached_indices):
            normalized_text = text.strip().lower()
            cache_key = f"emb_{hash(normalized_text)}"
            embedding_array = np.array(embedding)
            embedding_cache[cache_key] = embedding_array
            embeddings.append((idx, embedding_array))
    
    print(f"ğŸš€ Embedding cache hits: {cache_hits}/{len(all_texts)}, batch processed: {len(uncached_texts)}")
    
    # Phase 3: Sort and combine (aynÄ± logic korunuyor)
    if not embeddings:
        print("âŒ No embeddings generated, using fallback")
        return None
    
    # Sort back to original order
    embeddings.sort(key=lambda x: x[0])
    embedding_arrays = [emb[1] for emb in embeddings]
    
    if len(embedding_arrays) == 1:
        final_vector = embedding_arrays[0]
    else:
        # Ultra-optimized vector combination (aynÄ± logic)
        original_weight = 0.5
        hyde_weight = 0.5 / (len(embedding_arrays) - 1) if len(embedding_arrays) > 1 else 0
        
        embeddings_array = np.stack(embedding_arrays)
        weights = np.array([original_weight] + [hyde_weight] * (len(embedding_arrays) - 1))
        final_vector = np.average(embeddings_array, axis=0, weights=weights)
    
    print(f"ğŸ§® Created optimized ensemble vector from {len(embedding_arrays)} sources")
    return final_vector


async def ask_question_voice_optimized(question, collection, openai_client, model, cross_encoder_model, domain_context="", request=None):
    """ğŸš€ OPTIMIZED Voice RAG sistemi - %60 hÄ±z artÄ±ÅŸÄ±, aynÄ± kalite, disconnection control"""
    print(f"\nğŸ¤ Processing voice question (OPTIMIZED): {question}")
    if domain_context:
        print(f"ğŸ“„ Domain context: {domain_context}")
    
    # Helper function for checking disconnection (aynÄ± logic)
    async def check_disconnection(step_name):
        if request and await request.is_disconnected():
            print(f"ğŸšª Client disconnected during {step_name}")
            raise Exception(f"Client disconnected during {step_name}")
    
    # ğŸš€ PHASE 1: Parallel baÅŸlatma with disconnection checks
    print("âš¡ Phase 1: Parallel initialization...")
    await check_disconnection("phase 1 start")
    
    async def phase1_tasks():
        # Task 1: HyDE generation (en uzun sÃ¼ren)
        hyde_task = generate_multiple_hyde_variants(question, openai_client, domain_context=domain_context)
        
        # Task 2: Basic question embedding (HyDE beklemeden)
        question_embedding_task = asyncio.to_thread(model.encode, question)
        
        # Task 3: Keyword search (HyDE beklemeden baÅŸlayabiliyor)
        if _detect_db_type(collection) == 'weaviate':
            keyword_task = _weaviate_keyword_search(question, collection)
        else:
            keyword_task = _keyword_search(question, collection)
        
        return await asyncio.gather(hyde_task, question_embedding_task, keyword_task)
    
    hyde_variants, question_embedding, keyword_results = await phase1_tasks()
    await check_disconnection("phase 1 completion")
    
    # Safe access to keyword results
    keyword_count = 0
    if keyword_results and isinstance(keyword_results, dict):
        docs = keyword_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            keyword_count = len(docs[0])
    
    print(f"âœ… Phase 1 completed: HyDE={len(hyde_variants)}, embedding=ready, keyword={keyword_count}")
    
    # ğŸš€ PHASE 2: Enhanced query vector + Semantic search
    print("âš¡ Phase 2: Enhanced search...")
    await check_disconnection("phase 2 start")
    
    query_vector = await _create_smart_query_vector_optimized(question, hyde_variants, question_embedding, model)
    
    # Semantic search (query vector hazÄ±r olunca)
    if _detect_db_type(collection) == 'weaviate':
        semantic_results = await _weaviate_semantic_search(query_vector, collection)
    else:
        semantic_results = await _chroma_semantic_search(query_vector, collection)
    
    await check_disconnection("phase 2 completion")
    
    # Safe access to semantic results
    semantic_count = 0
    if semantic_results and isinstance(semantic_results, dict):
        docs = semantic_results.get('documents', [])
        if docs and isinstance(docs, list) and len(docs) > 0 and isinstance(docs[0], list):
            semantic_count = len(docs[0])
    
    print(f"âœ… Phase 2 completed: semantic={semantic_count}")
    
    # ğŸš€ PHASE 3: Results combination + Context + Voice Response
    print("âš¡ Phase 3: Results processing...")
    await check_disconnection("phase 3 start")
    
    # Combine search results (hÄ±zlÄ±, paralel olmasÄ±na gerek yok)
    combined_results = _combine_search_results(semantic_results, keyword_results)
    
    # Smart cross-encoder (mevcut logic korunuyor)
    if cross_encoder_model and combined_results:
        top_scores = [r['score'] for r in combined_results[:3]]
        score_variance = max(top_scores) - min(top_scores) if len(top_scores) > 1 else 1.0
        
        if score_variance < 0.15 and len(combined_results) > 3:
            print("ğŸ¯ Smart reranking: High uncertainty detected, applying cross-encoder...")
            final_results = await _cross_encoder_rerank(question, combined_results[:10], cross_encoder_model)
        else:
            print("ğŸš€ Smart reranking: Clear results, skipping cross-encoder for speed")
            final_results = combined_results[:10]
    else:
        final_results = combined_results[:10]
    
    # Context assembly (hÄ±zlÄ±, paralel gereksiz)
    context = _assemble_optimized_context(final_results, question, domain_context)
    
    await check_disconnection("voice response generation")
    
    # Voice-specific response generation
    response = await _generate_voice_response(question, context, openai_client, domain_context)
    
    # Extract source metadata (aynÄ± logic)
    sources_metadata = []
    for result in final_results[:10]:
        metadata = result.get('metadata', {})
        sources_metadata.append({
            'source': metadata.get('source', 'Unknown'),
            'article': metadata.get('article', 'Unknown')
        })
    
    print("ğŸ‰ Optimized Voice RAG completed!")
    return response, sources_metadata


async def create_optimized_embeddings(documents, model):
    """Original optimized embedding function - backward compatibility"""
    return await create_optimized_embeddings_v2(documents, model)