import os
import re
import pypdf
from tqdm import tqdm
from langchain_text_splitters import RecursiveCharacterTextSplitter
import google.generativeai as genai
import google.generativeai as genai
from config import MAX_CHARS_PER_CHUNK
import concurrent.futures
import numpy as np
from typing import List, Dict, Tuple
import asyncio

class SemanticDocumentProcessor:
    """Advanced semantic document processor with content-aware chunking"""
    
    def __init__(self, embedding_model=None):
        self.embedding_model = embedding_model
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=MAX_CHARS_PER_CHUNK,
            chunk_overlap=300,  # Increased overlap for better continuity
            separators=["\n\n", "\n", ". ", "? ", "! ", "; ", ", ", " "]
        )
        
        # Semantic patterns for Turkish documents
        self.structure_patterns = {
            'section': [
                r'BÃ–LÃœM\s+([IVX]+|\d+)(?:\s*[-â€“â€”]\s*(.+?))?',
                r'KISIM\s+([IVX]+|\d+)(?:\s*[-â€“â€”]\s*(.+?))?',
                r'CHAPTER\s+(\d+)(?:\s*[-â€“â€”]\s*(.+?))?'
            ],
            'article': [
                r'MADDE\s+(\d+)(?:\s*[-â€“â€”]\s*(.+?))?',
                r'Article\s+(\d+)(?:\s*[-â€“â€”]\s*(.+?))?',
                r'Madde\s+(\d+)(?:\s*[-â€“â€”]\s*(.+?))?'
            ],
            'subsection': [
                r'\((\d+)\)',  # (1), (2), etc.
                r'([a-z])\)',  # a), b), etc.
                r'(\d+)\)',    # 1), 2), etc.
            ],
            'list_item': [
                r'[-â€¢â–ªâ–«â—¦â–¸â–¹â–ª]\s+',  # Bullet points
                r'\d+\.\s+',       # Numbered lists
                r'[a-z]\.\s+',     # Letter lists
            ]
        }
    
    def detect_document_structure(self, text: str) -> Dict[str, List[Tuple[int, str]]]:
        """Detect hierarchical structure in document"""
        structure = {
            'sections': [],
            'articles': [],
            'subsections': [],
            'lists': []
        }
        
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line_clean = line.strip()
            if not line_clean:
                continue
                
            # Check for sections
            for pattern in self.structure_patterns['section']:
                match = re.match(pattern, line_clean, re.IGNORECASE)
                if match:
                    structure['sections'].append((i, line_clean))
                    break
            
            # Check for articles
            for pattern in self.structure_patterns['article']:
                match = re.match(pattern, line_clean, re.IGNORECASE)
                if match:
                    structure['articles'].append((i, line_clean))
                    break
            
            # Check for subsections
            for pattern in self.structure_patterns['subsection']:
                match = re.match(pattern, line_clean)
                if match:
                    structure['subsections'].append((i, line_clean))
                    break
            
            # Check for lists
            for pattern in self.structure_patterns['list_item']:
                match = re.match(pattern, line_clean)
                if match:
                    structure['lists'].append((i, line_clean))
                    break
        
        return structure
    
    def semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between two texts"""
        if not self.embedding_model:
            # Fallback: simple word overlap
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            return intersection / union if union > 0 else 0.0
        
        try:
            embeddings = genai.embed_content(model=self.embedding_model, content=[text1, text2], task_type="retrieval_document")['embedding']
            similarity = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
            )
            return float(similarity)
        except:
            return 0.0
    
    def create_semantic_chunks(self, text: str, structure: Dict) -> List[Dict]:
        """Create semantically coherent chunks based on document structure"""
        chunks = []
        
        # Special handling for short but info-dense documents
        if len(text) < 1500 and self._is_biographical_content(text):
            print("ðŸ“‹ Detected biographical content - applying key information extraction")
            bio_chunks = self._extract_biographical_chunks(text)
            if bio_chunks:
                chunks.extend(bio_chunks)
                return chunks
        
        # If document has clear structure, use it
        if structure['articles'] or structure['sections']:
            chunks.extend(self._create_structured_chunks(text, structure))
        else:
            # Fallback to content-based semantic chunking
            chunks.extend(self._create_content_based_chunks(text))
        
        return chunks
    
    def _is_biographical_content(self, text: str) -> bool:
        """Detect if content is biographical/profile information"""
        text_lower = text.lower()
        
        # Biographical indicators
        bio_indicators = [
            'rektÃ¶r', 'dekan', 'baÅŸkan', 'prof', 'dr',
            'doÄŸdu', 'mezun', 'atandÄ±', 'gÃ¶rev',
            'Ã¼niversite', 'akademik', 'eÄŸitim'
        ]
        
        # Count indicators
        indicator_count = sum(1 for indicator in bio_indicators if indicator in text_lower)
        
        # If many biographical indicators present
        return indicator_count >= 4
    
    def _extract_biographical_chunks(self, text: str) -> List[Dict]:
        """Extract key biographical information into separate chunks"""
        chunks = []
        
        # Split text into sentences
        import re
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # Group sentences by topic
        basic_info = []
        academic_info = []
        position_info = []
        publications = []
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # Position/title information
            if any(word in sentence_lower for word in ['rektÃ¶r', 'dekan', 'baÅŸkan', 'atandÄ±', 'gÃ¶rev']):
                position_info.append(sentence)
            # Academic background
            elif any(word in sentence_lower for word in ['Ã¼niversite', 'doktora', 'master', 'mezun', 'eÄŸitim']):
                academic_info.append(sentence)
            # Publications/works
            elif any(word in sentence_lower for word in ['kitap', 'makale', 'yayÄ±n', 'Ã§eviri', 'novel']):
                publications.append(sentence)
            # Basic personal info
            elif any(word in sentence_lower for word in ['doÄŸdu', 'yÄ±l', 'tarih']):
                basic_info.append(sentence)
            else:
                # Default to academic info if unsure
                academic_info.append(sentence)
        
        # Create chunks for each category that has content
        if position_info:
            chunks.append({
                'content': ' '.join(position_info).strip(),
                'structure_type': 'biographical',
                'title': 'GÃ¶rev ve Pozisyon Bilgileri',
                'hierarchy_level': 1
            })
        
        if basic_info:
            chunks.append({
                'content': ' '.join(basic_info).strip(),
                'structure_type': 'biographical', 
                'title': 'KiÅŸisel Bilgiler',
                'hierarchy_level': 1
            })
        
        if academic_info:
            chunks.append({
                'content': ' '.join(academic_info).strip(),
                'structure_type': 'biographical',
                'title': 'Akademik GeÃ§miÅŸ',
                'hierarchy_level': 1
            })
        
        if publications:
            chunks.append({
                'content': ' '.join(publications).strip(),
                'structure_type': 'biographical',
                'title': 'YayÄ±nlar ve Eserler', 
                'hierarchy_level': 1
            })
        
        # If no meaningful categorization possible, return original as single chunk
        if not chunks:
            chunks.append({
                'content': text.strip(),
                'structure_type': 'biographical',
                'title': 'Biyografik Bilgiler',
                'hierarchy_level': 1
            })
        
        print(f"   ðŸ“Š Extracted {len(chunks)} biographical chunks")
        return chunks
    
    def _create_structured_chunks(self, text: str, structure: Dict) -> List[Dict]:
        """Create chunks based on detected document structure"""
        chunks = []
        lines = text.split('\n')
        
        # Smart primary structure selection
        sections = structure['sections']
        articles = structure['articles']
        
        # If we have many articles but few sections, prefer articles
        if len(articles) > len(sections) * 3:
            primary_markers = articles
            structure_type = 'article'
        elif sections:
            primary_markers = sections
            structure_type = 'section'
        elif articles:
            primary_markers = articles
            structure_type = 'article'
        else:
            return self._create_content_based_chunks(text)
        
        print(f"ðŸ“Š Using {structure_type} markers: {len(primary_markers)} items")
        
        for i, (line_idx, marker_text) in enumerate(primary_markers):
            # Determine chunk boundaries
            start_line = line_idx
            
            # Find end of this section/article
            if i + 1 < len(primary_markers):
                end_line = primary_markers[i + 1][0]
            else:
                end_line = len(lines)
            
            # Extract content
            section_lines = lines[start_line:end_line]
            section_text = '\n'.join(section_lines).strip()
            
            if not section_text:
                continue
            
            # If section is too large, split it further
            if len(section_text) > MAX_CHARS_PER_CHUNK * 1.5:
                sub_chunks = self._split_large_section(section_text, marker_text)
                chunks.extend(sub_chunks)
                print(f"ðŸ“„ Split large {structure_type}: {marker_text[:50]}... into {len(sub_chunks)} sub-chunks")
            else:
                chunks.append({
                    'content': section_text,
                    'structure_type': structure_type,
                    'title': marker_text,
                    'hierarchy_level': 1
                })
        
        return chunks
    
    def _split_large_section(self, text: str, title: str) -> List[Dict]:
        """Split large sections into smaller, more manageable chunks using a robust text splitter."""
        chunks = []
        
        # Directly use the configured RecursiveCharacterTextSplitter for efficient chunking
        fallback_chunks = self.text_splitter.split_text(text)
        
        for i, chunk_text in enumerate(fallback_chunks):
            chunks.append({
                'content': chunk_text,
                'structure_type': 'subsection',
                'title': f"{title} (BÃ¶lÃ¼m {i+1})",
                'hierarchy_level': 2
            })
            
        return chunks
    
    def _create_content_based_chunks(self, text: str) -> List[Dict]:
        """Create chunks based on content similarity and natural boundaries"""
        # Use RecursiveCharacterTextSplitter as base
        base_chunks = self.text_splitter.split_text(text)
        
        if not base_chunks:
            return []
        
        # If we have an embedding model, merge similar adjacent chunks
        if self.embedding_model and len(base_chunks) > 1:
            merged_chunks = self._merge_similar_chunks(base_chunks)
        else:
            merged_chunks = base_chunks
        
        # Convert to structured format
        structured_chunks = []
        for i, chunk in enumerate(merged_chunks):
            # Try to extract a meaningful title from the chunk
            title = self._extract_chunk_title(chunk)
            
            structured_chunks.append({
                'content': chunk,
                'structure_type': 'content',
                'title': title or f"Tam Metin (BÃ¶lÃ¼m {i+1})",
                'hierarchy_level': 1
            })
        
        return structured_chunks
    
    def _merge_similar_chunks(self, chunks: List[str], similarity_threshold: float = 0.6) -> List[str]:
        """Merge adjacent chunks that are semantically similar"""
        if len(chunks) <= 1:
            return chunks
        
        merged = []
        current_chunk = chunks[0]
        
        for i in range(1, len(chunks)):
            next_chunk = chunks[i]
            
            # Calculate similarity between current and next chunk
            similarity = self.semantic_similarity(current_chunk[-500:], next_chunk[:500])
            
            # Merge if similar and combined size is reasonable
            combined_size = len(current_chunk) + len(next_chunk)
            
            if similarity > similarity_threshold and combined_size <= MAX_CHARS_PER_CHUNK * 1.3:
                current_chunk = current_chunk + '\n\n' + next_chunk
            else:
                merged.append(current_chunk)
                current_chunk = next_chunk
        
        # Add the last chunk
        merged.append(current_chunk)
        return merged
    
    def _extract_chunk_title(self, chunk: str) -> str:
        """Extract a meaningful title from chunk content"""
        lines = chunk.split('\n')
        
        for line in lines[:3]:  # Check first 3 lines
            line = line.strip()
            if not line:
                continue
            
            # Check if it looks like a title (short, capitalized, etc.)
            if (len(line) < 100 and 
                (line.isupper() or 
                 line.startswith(('MADDE', 'BÃ–LÃœM', 'KISIM', 'Article', 'Chapter')) or
                 re.match(r'^[A-Z][a-z].*[^.]$', line))):
                return line
        
        # Fallback: use first meaningful sentence
        sentences = re.split(r'[.!?]', chunk)
        for sentence in sentences[:2]:
            sentence = sentence.strip()
            if 20 <= len(sentence) <= 100:
                return sentence[:80] + "..." if len(sentence) > 80 else sentence
        
        return None

def _process_single_file_semantic(file_path, processor):
    """Process a single file with semantic awareness"""
    filename = os.path.basename(file_path)
    docs = []
    
    try:
        reader = pypdf.PdfReader(file_path)
        full_text = ""
        
        for page in reader.pages:
            page_text = page.extract_text() or ""
            full_text += page_text
        
        if not full_text.strip():
            print(f"Warning: No text extracted from {filename}, using filename as content")
            full_text = f"Document: {filename}\nContent could not be extracted from this PDF file."

        # Detect document structure
        structure = processor.detect_document_structure(full_text)
        
        # Create semantic chunks
        semantic_chunks = processor.create_semantic_chunks(full_text, structure)
        
        # Convert to standard format
        for chunk_data in semantic_chunks:
            docs.append({
                "source": filename,
                "article": chunk_data['title'],
                "content": chunk_data['content'],
                "structure_type": chunk_data['structure_type'],
                "hierarchy_level": chunk_data['hierarchy_level']
            })
        
        print(f"ðŸ“„ {filename}: {len(docs)} semantic chunks, structure: {len(structure['articles'])} articles, {len(structure['sections'])} sections")
        
    except Exception as e:
        print(f"Error processing file {filename}: {e}")
        # Fallback chunk
        docs.append({
            "source": filename,
            "article": "Error Recovery",
            "content": f"Document: {filename}\nProcessing error: {str(e)}",
            "structure_type": "error",
            "hierarchy_level": 0
        })
    
    return docs

def load_and_process_documents(directory_path, specific_files=None, embedding_model=None):
    """Load and process documents with semantic awareness"""
    all_docs = []
    
    if not os.path.exists(directory_path):
        return []
    
    # Initialize semantic processor
    processor = SemanticDocumentProcessor(embedding_model)
    
    filenames_to_process = specific_files if specific_files is not None else [
        f for f in os.listdir(directory_path) if f.endswith(".pdf")
    ]
    
    if not filenames_to_process:
        return []

    file_paths = [os.path.join(directory_path, f) for f in filenames_to_process]
    
    print(f"ðŸ§  Processing {len(file_paths)} documents with semantic chunking...")

    with concurrent.futures.ThreadPoolExecutor() as executor:
        future_to_path = {
            executor.submit(_process_single_file_semantic, path, processor): path 
            for path in file_paths
        }
        
        for future in tqdm(concurrent.futures.as_completed(future_to_path), 
                          total=len(file_paths), desc="Semantic Processing"):
            all_docs.extend(future.result())
            
    print(f"âœ… Semantic processing complete: {len(all_docs)} total chunks")
    return all_docs 